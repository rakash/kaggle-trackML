{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01baac3757b22b92747599ed87524c1ddc0ad1d"
   },
   "source": [
    "# Ultrafast sparse binning clustering enhanced by ML track scoring\n",
    "\n",
    "###  Yuval Reina, Tel-Aviv Israel, [Yuval.reina@gmail.com](Yuval.reina@gmail.com)\n",
    "###  Trian Xylouris, Frankfurt am Main Germany, [t.xylouris@gmail.com](t.xylouris@gmail.com)\n",
    "\n",
    "August 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competition Name: TrackML\n",
    "#### Team Name: Yuval & Trian\n",
    "#### Private Leaderboard Score: 0.80414\n",
    "#### Private Leaderboard Place: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We use sparse binning to perform ultrafast clustering. Tracks are first chosen according to their length, and later are scored and merged using a Machine Learning (=ML) algorithm. At the final stage the tracks are expanded by adding the closest hits to the track. We describe our method in Chapter 1. In Chapter 2, we complete one full, yet quick, code run for training event 1000, which yields a 0.75 score.\n",
    "\n",
    "**The biggest advantage of using clustering by sparse binning is speed**. This method can score 0.5 in just 40 sec using python on a single core, and it could be much faster if it was written in C++ and even faster by using parallel computing on a CPU or GPU. **The computational complexity of the clustering part in the algorithm is O(N)**. The feature calculation and binning can be done for every hit independently of the other hits and all hits are needed only for counting the number of hits in every bin (the Python implementation uses np.unique which is actually O(N logN)  ). We believe that with a careful implementation in C++  a score of 0.5 can be achieved in less than 10 seconds, allowing this algorithm to be an essential step in every fast algorithm (we will explain this statement later).\n",
    "\n",
    "The second stage in the algorithm is ML merging. The fastest way to merge 2 solutions is by assigning to each hit the track with the highest number of hits. This method is used in the clustering main loop. However, the number of hits is not always a good indication for a good track. The ML algorithm uses various features which describe the track and is able to distinguish between good and bad tracks. Unlike other ML solutions presented in the TrackML Kaggle competition, our ML algorithm does not check the helix itself, as the clustering part already takes care of this.\n",
    "\n",
    "The last part in the algorithm is quite straightforward: track expanding is done by selecting long tracks and adding to these tracks the closest loose hits (i.e. hits from short tracks). \n",
    "\n",
    "The full solution is then: \n",
    "- Run clustering a few times\n",
    "- Use ML to merge\n",
    "- Expand \n",
    "\n",
    "In the solution we submitted, we used about 7 runs of clustering, each using 100000 loop iterations, merged and expanded twice (the 2nd time was just to add about 0.003 to the score by using a loophole in the definition of the competition metric [(1)](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Methods and Results\n",
    "\n",
    "## Clustering using Sparse Binning\n",
    "\n",
    "The basic idea of clustering it to relate to every hit a set of numbers (features) and then to group together hits whose features are close to each other. The first stage in clustering is finding features which describe a track well and which we can calculate separately for each hit. We have published a post on criteria for good features [(2)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590).\n",
    "\n",
    "### Features \n",
    "Due to the magnetic field in the experiment the particles form helixes, which travel along the z-axis. Each hit in the x-y-z space can be the member of any particular helix in a certain family of helixes. The idea of the algorithm is to go over each possible member of this family of helixes, and look at how many of all hits can belong to this particular member. The hits are then assigned to the helix (=track) with the most possible hits.\n",
    "\n",
    "We start by assuming the particle is formed in a small cylinder around the origin i.e. the approximate starting point of the helix is $(0,0,z0)$. According to the introduction papers, we usually have $|z0|<5.5mm$.\n",
    "\n",
    "This kind of helix can be defined by 3 numbers: its radius, its tangential angle at the origin in the xy plane (=the direction of the particle when it is created, in xy plane) and the slope of the helix (how fast the particle moves in the z direction compared to its velocity in xy).\n",
    "\n",
    "Let\n",
    "\n",
    "$R =$ helix radius,\n",
    "\n",
    "$\\theta$ = tangential angle in the xy plane,\n",
    "\n",
    "$\\phi$ = slope,\n",
    "\n",
    "$(px, py, pz) =$ particle’s initial momentum.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta = \\arctan\\frac{py}{px} \\label{x1}, \\\\\n",
    "&\\phi = \\arctan\\frac{pz}{\\sqrt{px^2+py^2}} \\label{x2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For each hit $(x,y,z)$ we can calculate its values for $\\theta$ and $\\phi$, given $R$ and $z0$. We define\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&kt = \\frac{1}{2R} \\label{x3} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We will iterate over $kt$. If $kt>0$, then we deal with particles rotating clockwise and $kt<0$ for counter-clockwise rotation. Using some trigonometry, we get:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta\\_ = \\arctan\\frac{y}{x}\\label{x4} \\\\\n",
    "\\\\\n",
    "&rr=\\sqrt{x^2+y^2} \\label{x5} \\\\\n",
    "\\\\\n",
    "&\\Delta\\theta=\\arcsin{(kt\\cdot rr)} \\label{x6} \\\\\n",
    "\\\\\n",
    "&\\theta=\\theta\\_+\\Delta\\theta \\label{x7}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "(At this stage we ignore particles that rotate more then $\\pi$ radians)\n",
    "$$ \\begin{align*}\n",
    "&\\phi\\_ = \\arctan\\frac{(z-z0)\\cdot kt}{\\Delta\\theta} \\label{x8}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\theta$ and $\\phi\\_$ will be our features, but we need to tweak them a little to become useful.\n",
    "\n",
    "First, we are only interested in the value of $\\theta$ modulo $2\\pi$. Thus, instead of $\\theta$, we use the two features:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&sint = \\sin(\\theta) \\label{x9}\\\\\n",
    "&cost = \\cos(\\theta) \\label{x10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\phi\\_$ on the other hand is in the range $[-\\pi/2,\\pi/2]$, and does not have the above issue, but $\\phi\\_$ distribution is far from being uniform (for an extensive discussion about its distribution look here [(3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-371940)). \n",
    "\n",
    "The solution we found suitable is to use the following instead of $\\phi\\_$:\n",
    "$$ \\begin{align*}\n",
    "&\\phi = \\arctan\\frac{(z-z0)\\cdot kt}{3.3\\cdot\\Delta\\theta}\\frac{2}{\\pi} \\label{x11} .\n",
    "\\end{align*}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Binning Clustering\n",
    "Fix a pair $(kt,z0)$. We calculate the three afore-mentioned features for each hit. Now, binning is the easiest way to cluster together different hits, based on whether its features are near each other.\n",
    "\n",
    "To illustrate how binning works, suppose that for all hits, a certain feature of those hit lies always in the range $[-1,1]$. We may then divide this interval into 20 bins of equal size $0.1$:\n",
    "\n",
    "$$[-1,-0.9), [-0.9,0.8), …,[0.9,1].$$ \n",
    "\n",
    "We assign a number 0 , …, 19 to every bin. If we have three features for each hit, then we assign to each hit a tuple with three values: One bin number for each of its three features:\n",
    "\n",
    "`\n",
    "Hit #1 – (12,15,0)\n",
    "Hit #2 – (11,5,2)\n",
    "...`\n",
    "\n",
    "Hits which have the exact same three values are clustered together to form one track candidate.\n",
    "\n",
    "The number of bins for each feature, as well as the size of each bin, may vary. In our Sparse Binning, we use a very simple binning algorithm. We start by using uniformly spread bins which can be easily calculated:\n",
    "\n",
    "For feature $F_j$ in the range $[-1,1]$, we choose to have $2*k_j$ bins, and define $B_j = int(F_j*k_j)$.\n",
    "\n",
    "To combine several features together, and get the cluster id (=track_id), we just multiply each $B_j$ with a different, large enough, number, and add the resulting values. For instance, if we have two features $F_1$, $F_2$ and we want to have $2*k_1$, $2*k_2$ bins for each feature, we can define:\n",
    "\n",
    "$$ (a) TrackId = int(F_1*k_1) + (2*k_1+1)*int(F_2*k_2)$$\n",
    "\n",
    "And that’s it. Clustering is finished!\n",
    "\n",
    "This calculation takes about 4mSec for 120000 hits on the Kaggle kernel platform compared to about 950mSec for dbscan, which is the most popular clustering algorithm used by the competitors in the TrackML challenge. \n",
    "\n",
    "This type of clustering moves the bottleneck of the full solution form clustering to merging (will be discussed in the next section).\n",
    "\n",
    "One weakness of the binning algorithm is its sensitivity, caused - among other things - by the hard borders between bins. As an example, let’s take two hits:\n",
    "\n",
    "`\n",
    "Hit1 – with features (-1e-7,0.1,0.3)\n",
    "Hit2 – with features (1e-7,0.1,0.3)`\n",
    "\n",
    "If we use the calculation above, Hit1 and Hit2 will never be clustered together, no matter how many bins we have. This issue is solved by adding a random number $r_j$ in the range $[0,1)$ to every feature in the binning calculation, turning (a) to:\n",
    "\n",
    "$$(b) TrackId = int(F_1*k_1+r_1) + (2*k_1+1)*int(F_2*k_2+r_2) .$$\n",
    "\n",
    "Here, $r_1$, $r_2$ are random numbers with uniform distribution in the range $[0,1)$. We change them for every pair $(kt, z0)$.\n",
    "\n",
    "When compared to dbscan, sparse clustering has one big disadvantage, it’s sensitivity. It can miss some of the hits because they fall in another bin. This sensitivity is also its strength, as it doesn’t add many wrong hits to a track (outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Merging\n",
    "Every time we perform a clustering, using a pair $(kt, z0)$, every hit gets a new track_id, thus generating one new list with track ids. We need to merge two lists at a time. The most efficient way we found is to measure the length of each track in the two lists, and let every hit choose the longest track it can.\n",
    "\n",
    "If Hit #1 has track_id=1 in list #100 and track_id=10000002 in list #101 (the track_ids in both lists should be completely different) and in list #100 there are 10 hits with track_id=1 while in list #101 there are 7 hits with track_id=10000002, then hit#1 will be assigned to track_id=1.\n",
    "\n",
    "To do this, we need to measure the track’s length. The fastest way we know to do this in Python is by using numpy.unique. This operation takes 11mSec for 120000 hits on the Kaggle kernel platform. The numpy.unique computational complexity is O(NlogN), because it sorts the values. However, if we have enough memory, this task can easily be done in O(N). Also, we can reduce the amount of memory needed by hashing. \n",
    "\n",
    "In our main loop we measure the length of a track twice. The first time in order to create a track candidate and the second time to update the final track length for each track (and their related hits), as the latter is used to merge the next list of track ids onto the current one.\n",
    "\n",
    "The double track length measurement makes this the bottleneck of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering – the full algorithm\n",
    "The algorithm iterates over randomly selected pairs $(kt,z0)$. For every pair, the hits’ features are calculated, and clustering is performed by the sparse binning algorithm. The new clustering result is then merged with previous ones with the merging algorithm described above. \n",
    "\n",
    "Every 500 loops an extra step is taken to ensure that the following two conditions are met:\n",
    "- at most 1 hit from the same detector (equal Volume_id, Layer_id, and Module_id) can belong to one track\n",
    "- at most 2 hits from the same layer (equal Volume_id and Layer_id) can belong to one track\n",
    "We remove hits, until those condition are met. The hits, which are removed, are selected based on how far their features are from the means of the other hits' features.\n",
    "\n",
    "In this last step, the algorithm also carves out \"good\" tracks which are long enough (hits won’t be taken away or added to this track anymore).\n",
    "\n",
    "While running the algorithm, we change the number of bins per feature and the minimal length for tracks to be carved out. These settings are user-defined parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering results:\n",
    "The more pairs $(kt,z0)$ we iterate over, the better the final score achieved by clustering (below scores measured on event 000001000 from the training set – the score is usually 0.015 below the final leaderboard score):\n",
    "\n",
    "|   Number of pairs   |  Score |\n",
    "|---------------------|--------|\n",
    "|        1000\t      |  0.51  |\n",
    "|        1600\t      |  0.56  |\n",
    "|        5500\t      |  0.636 |\n",
    "|      100000\t      |  0.73  |\n",
    "\n",
    "The score plateaus after about 90000 pairs and doesn’t increase further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning\n",
    "Running the clustering algorithm a few times will produce similar, but slightly different solutions. Every one of these solutions has some good tracks that the other solution missed. Trying to merge these solutions in a naive way, by selecting the longest track for each hit, does not improve the score when the initial scores are good enough.\n",
    "\n",
    "In our solution we built an algorithm to merge several solutions by using a Machine Learning (=ML) algorithm to evaluate the quality of the tracks. There were some discussions on taking a similar approach [(4)](https://www.kaggle.com/c/trackml-particle-identification/discussion/58323), but we don’t know of anyone who implemented it. As far as we know, most ML implementations where in an effort to construct or expand the tracks. \n",
    "\n",
    "The nice thing about this kind of implementation is the fact that it is quite agnostic to the track construction algorithm. Hence it can be used to merge tracks from completely different algorithms. \n",
    "\n",
    "### General strategy\n",
    "The steps are as follows: \n",
    "- Produce different submission candidates sub_1, sub_2, ..., sub_N\n",
    "- Create a machine learning model, which gives probabilities between 0 and 1 for each track candidate\n",
    "- Merge two submission candidates by assigning to each hit the track, which has higher probability. Actually, we add to the probabilities a fraction of the track-length, and after a couple of merges, we also ask the new probability (from sub_j) to be at least $0.5$ higher than the existing one\n",
    "- Merge all submission candidates to get the final submission. The merging can be done sequentially, as we did in our final submission \n",
    "\n",
    "### Creating the machine learning model\n",
    "The machine learning model we use is LightGBM. We choose 13 features per track:\n",
    "- variance of x,y,z (these are the most important)\n",
    "- minimum of x,y,z\n",
    "- maximum of x,y,z\n",
    "- mean of z\n",
    "- volume_id of first hit\n",
    "- number of clusters per track (i.e. are there many hits, which are close together?)\n",
    "- number of hits divided by number of clusters\n",
    "\n",
    "\n",
    "\n",
    "### Training and validation\n",
    "\n",
    "Training data from roughly 250 training events with roughly 5 million tracks were used: \n",
    "- Correct tracks (target=1): All true tracks, taken from truth files\n",
    "- Wrong tracks (target=0): These were generated by first running the clustering algorithm on an event, and then picking all generated tracks, whose hits belonged to at least two different particles (particle_id)\n",
    "\n",
    "Good results were already achieved with 50000 training tracks, with diminishing gains after that. Training was a matter of minutes (some parameters we used: LightGBM with 3000 steps, learning_rate=0.05, 128 leafs). We also tested giving a \"purity score\" to each track (using \"objective=xentropy\" instead of \"binary\" in LightGBM), depending on how many percent of the suggested track belongs to the same particle. This did not change things enough to be further considered.\n",
    "\n",
    "We tuned the hyperparameters of the model, based on the 3 training events with the ids 0, 1 and 2, which formed our validation set.\n",
    "\n",
    "On the validation set, we achieved strong rates of roughly 95% for precision, accuracy and recall. However, these rates did not translate into big gains for the final score. One reason may be that the used correct/wrong tracks of our training/validation data did not resemble the final situation well enough, at which the model was employed. One step to improve this, was to take the \"purity score\" as described above, but this did not help our implementation.\n",
    "\n",
    "Note: For our Machine Learning model to be helpful, it needs to distinguish correct from wrong tracks with very high \n",
    "\n",
    "$$precision=TruePositives/(FalsePositives+TruePositives).$$ \n",
    "\n",
    "Also, it needs to do so for various sets of track candidates, especially those, which are generated if one tries to find tracks which originate far away from the origin. In these latter situations, often a lot of bad candidates are produced.\n",
    "\n",
    "\n",
    "### Approaches, which did not yield benefits\n",
    "We tried dozens of other features (e.g. number of different volumes crossed, means of x,y), but got only negligible gains, probably because these features are closely related to the features we already use. \n",
    "\n",
    "We also tried a deep neural network with a few hidden layers (and embeddings) and an LSTM architecture (where the input is a sequence of one value for each of the up to 20 hits of a track; the value was e.g. the volume-layer-module id). The first model gave similar performance to LightGBM, while the described LSTM model gave slightly worse results, though still better than we expected by just using the volume-layer-module id.\n",
    "\n",
    "Finally, we also tried a scikit random forest implementation and Logistic Regression, but both gave worse results.\n",
    "\n",
    "\n",
    "### Results\n",
    "In our final solution, we used the algorithm to sequentially merge 7 clustering solutions (mostly from previously generated submissions). We were able to increase our score by about 0.01. After the completion of the competition, we did another test where we merge 64 fast clustering solutions, using 1000 $(kt,z0)$ pairs each (i.e. a total of 64000 pairs). The score we got after expanding, for train event 1000, was $0.782$. We get a similar score, but with longer runtime, by our usual algorithm of clustering 100000 pairs and then expanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Track Extension\n",
    "The major drawback of the binning clustering algorithm is its sensitivity. The bins' sharp borders can leave a hit out of the track, although it is very close to the other hits. To overcome this issue, we employ a track expanding algorithm as a final step in our solution.\n",
    "\n",
    "We start with a solution, which comes from merging different clustering solutions with our ML model. First, the track extension algorithm tries to improve the $(kt,z0)$ pair for each track. It does this by searching for a pair which will minimize the standard deviation of the track hits' features.\n",
    "\n",
    "Using these refined $(kt,z0)$ values, the hits which are closest to a track are added to it. The distance between a hit and a track is measured by the difference between its features and the mean of the tracks' features.\n",
    " \n",
    "We also tested a variant of this algorithm which measures the minimal distance to just one of the tracks' hits (compare nearest neighbor algorithm). This variant performed slightly better but was much slower.  \n",
    "\n",
    "The improvement gained by expanding depends on the score after the previous stages. We observed the following score improvements by using our track extension algorithm:\n",
    "\n",
    "- 0.63 increases to 0.73,\n",
    "- 0.73 increases to 0.79,\n",
    "- and in our final run, 0.78 increased to 0.804\n",
    "\n",
    "Finally we note, that expanding multiple times usually decreases the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "1.\tOur solution can be used in full, after being (easily) optimized for speed\n",
    "    -\tThe clustering algorithm can be improved as described above. Further improvements can be achieved by massively parallelizing the feature calculation using a GPU (as we explained above, every hit is treated separately, which makes the algorithm very suitable for parallel processing).\n",
    "    -\tBoth, the ML and the expanding algorithm where not optimized for speed, as they weren’t the bottleneck of the solution. As an example, the expanding algorithm recalculates the features for all the loose hits every time. Calculating the features only for hits with z>0 while expanding tracks with z>0 will immediately cut the running time of the algorithm in half. \n",
    "2.\tElements of our approach may be beneficial for any final solution\n",
    "   - In particular, our binning algorithm creates a lot of good candidate tracks in a short period of time. It may be helpful to employ this at the beginning of a solution, in order to very quickly (<1 minute) detect 50% of all tracks (use small bins). One can then continue with running a different algorithm on the remaining hits.\n",
    "   - Similarly, the machine learning algorithm, as well as the employed parameters and track extension algorithm may improve any final result, while just adding minutes (or less, after optimization) to the total runtime.\n",
    "3.\tIn our algorithm we didn’t do any adjustments for the uneven magnetic field. If we incorporate @CPMP’s findings [here (3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564), then our score improves immediately by 0.02 to about 0.82. \n",
    "4.\tAn obvious short-coming of our main approach is, that it is not suited to find tracks, which originate far from the origin. We did try to adjust our algorithm to work also in that situation, but had minimal success so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "(1)  @Grzegorz Sionkowski comment in *\"how to score a track is good or not\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)\n",
    "\n",
    "(2) *\"Criteria for good features\"* [(https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)\n",
    "\n",
    "(3) @CPMP *\"Solution #9\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564)\n",
    "\n",
    "(4) @Heng CherKeng *\"ensemble clustering?\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/58323](https://www.kaggle.com/c/trackml-particle-identification/discussion/58323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Running full pipeline for train event 1000\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We will demonstrate how to get a score of about 0.75 for train event 1000, by using our aforementioned method. The steps are: \n",
    "- Create 2 initial solutions: 2x 5500 pairs of $(kt,z0)$ for binning (in our original solution, we use 100000 pairs)\n",
    "- Merge the 2 solutions using our machine learning algorithm (in our original solution, we use 7-8)\n",
    "- Extend the tracks (In our original solution, we do this twice. At the first run, we extend hits according to whether their features are close enough to at least 1 hit, instead of the mean of the tracks' features.)\n",
    "\n",
    "We first import necessary packages and load train event 1000, which will be the event we will be working on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'other/')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from ipywidgets import FloatProgress,FloatText\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import gc\n",
    "import cProfile\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)})\n",
    "plt.figure(figsize=(12,5))\n",
    "path='files/'\n",
    "\n",
    "from functions.other import calc_features, get_event, score_event_fast, load_obj\n",
    "from functions.expand import *\n",
    "from functions.cluster import *\n",
    "from functions.ml_model import merge_with_probabilities,precision_and_recall,get_features,get_predictions\n",
    "\n",
    "# auto load changed modules:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "event_num=0\n",
    "event_prefix = 'event00000100{}'.format(event_num)\n",
    "hits, cells, particles, truth = get_event(path,event_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Define parameters and run clustering, twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "00cff1de64d7a7f24df2e9430344f5ae90dc953c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33fbfcc364b4d9c856d08696a89980f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e41c1a6b7c48e1866ead6c03976ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b27c4d39864f4b982a9e916aa17026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6594c35fec6f4e5b91745d8ddce79cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f83c8e73da453983d0e56bec9641bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626969ffecae49ef8a35a83b00402b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522b11cee95c410bb374e8875e982e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [03:48<00:00, 24.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 228.19511 sec\n",
      "Your score:  0.6362028679399999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f60348e9b34086837c76b7859c133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89ae19726fd4b1994c6f29d612302d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cf1250fb014d9795cd86d3892b3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7363750f84444483643e26bb70d811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41729ee23e8644af8f16b13fcb531c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f50171a6f74d389521f4d64f665634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5d84b467d4d938c10d7e2ef1721a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [03:48<00:00, 24.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 228.19628 sec\n",
      "Your score:  0.63804680154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "weights={'pi':1,'theta':0.15}\n",
    "stds={'z0':7.5, 'kt':7.5e-4}\n",
    "d =    {'sint':[225,110,110,110,110,110],\n",
    "        'cost':[225,110,110,110,110,110],\n",
    "          'phi':[550,260,260,260,260,260],\n",
    "        'min_group':[11,11,10,9,8,7],\n",
    "        'npoints':[500,2000,1000,1000,500,500]}\n",
    "filters=pd.DataFrame(d)\n",
    "nu=500\n",
    "\n",
    "resa1=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa1[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa1.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)\n",
    "\n",
    "resa2=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa2[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa2.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317879ec4824598fad62b7faba8f981eed065681"
   },
   "source": [
    "## Employ Machine Learning\n",
    "\n",
    "We have prepared a smaller sized training and test set. We load it directly from a pkl-file (compare \"Create training.ipynb\"). In particular, the below training data contains roughly 140k tracks, instead of the 5 million tracks, which we used to train the model of our original solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5800822ec8d5798c6bc9ecba2df5f20964fe13cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with all features:\n",
      "(138645, 15) (35636, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nclusters</th>\n",
       "      <th>nhitspercluster</th>\n",
       "      <th>svolume</th>\n",
       "      <th>target</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xvar</th>\n",
       "      <th>ymax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>yvar</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zmin</th>\n",
       "      <th>zvar</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35636</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>232.5970</td>\n",
       "      <td>65.19430</td>\n",
       "      <td>5382.914534</td>\n",
       "      <td>451.94500</td>\n",
       "      <td>31.1990</td>\n",
       "      <td>32042.402785</td>\n",
       "      <td>368.20000</td>\n",
       "      <td>171.902840</td>\n",
       "      <td>48.5054</td>\n",
       "      <td>17947.271693</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35637</th>\n",
       "      <td>5</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-161.7980</td>\n",
       "      <td>-284.92200</td>\n",
       "      <td>2356.988817</td>\n",
       "      <td>228.15800</td>\n",
       "      <td>55.5610</td>\n",
       "      <td>5201.288589</td>\n",
       "      <td>-242.99000</td>\n",
       "      <td>-448.451667</td>\n",
       "      <td>-565.6000</td>\n",
       "      <td>17366.214897</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35638</th>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>447.0080</td>\n",
       "      <td>4.62093</td>\n",
       "      <td>25546.189171</td>\n",
       "      <td>-32.21450</td>\n",
       "      <td>-912.0240</td>\n",
       "      <td>97570.763048</td>\n",
       "      <td>-9.43438</td>\n",
       "      <td>-53.372397</td>\n",
       "      <td>-86.8000</td>\n",
       "      <td>778.586234</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35639</th>\n",
       "      <td>10</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0611</td>\n",
       "      <td>1.76645</td>\n",
       "      <td>740.710959</td>\n",
       "      <td>404.80600</td>\n",
       "      <td>31.8856</td>\n",
       "      <td>13553.029720</td>\n",
       "      <td>2954.50000</td>\n",
       "      <td>1302.277917</td>\n",
       "      <td>226.3350</td>\n",
       "      <td>727483.387761</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35640</th>\n",
       "      <td>7</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>-33.0381</td>\n",
       "      <td>-465.40300</td>\n",
       "      <td>19380.462489</td>\n",
       "      <td>-2.26524</td>\n",
       "      <td>-180.0250</td>\n",
       "      <td>3385.299597</td>\n",
       "      <td>-34.30120</td>\n",
       "      <td>-208.410556</td>\n",
       "      <td>-526.4000</td>\n",
       "      <td>24820.227196</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nclusters  nhitspercluster  svolume  target      xmax       xmin  \\\n",
       "35636          5         1.000000        8       0  232.5970   65.19430   \n",
       "35637          5         1.200000        8       0 -161.7980 -284.92200   \n",
       "35638          6         1.000000        8       0  447.0080    4.62093   \n",
       "35639         10         1.200000        8       1   83.0611    1.76645   \n",
       "35640          7         1.285714        8       1  -33.0381 -465.40300   \n",
       "\n",
       "               xvar       ymax      ymin          yvar        zmax  \\\n",
       "35636   5382.914534  451.94500   31.1990  32042.402785   368.20000   \n",
       "35637   2356.988817  228.15800   55.5610   5201.288589  -242.99000   \n",
       "35638  25546.189171  -32.21450 -912.0240  97570.763048    -9.43438   \n",
       "35639    740.710959  404.80600   31.8856  13553.029720  2954.50000   \n",
       "35640  19380.462489   -2.26524 -180.0250   3385.299597   -34.30120   \n",
       "\n",
       "             zmean      zmin           zvar  event_id  \n",
       "35636   171.902840   48.5054   17947.271693         6  \n",
       "35637  -448.451667 -565.6000   17366.214897         6  \n",
       "35638   -53.372397  -86.8000     778.586234         6  \n",
       "35639  1302.277917  226.3350  727483.387761         6  \n",
       "35640  -208.410556 -526.4000   24820.227196         6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features/data for each track: ['nclusters' 'nhitspercluster' 'svolume' 'target' 'xmax' 'xmin' 'xvar'\n",
      " 'ymax' 'ymin' 'yvar' 'zmax' 'zmean' 'zmin' 'zvar' 'event_id']\n"
     ]
    }
   ],
   "source": [
    "df_train=load_obj('files/df_train_v2-reduced.pkl')\n",
    "df_test=load_obj('files/df_test_v1.pkl')\n",
    "y_train=df_train.target.values\n",
    "y_test=df_test.target.values\n",
    "print(\"The dataframe with all features:\")\n",
    "print(df_train.shape,df_test.shape)\n",
    "display(df_train.head())\n",
    "print(\"Features/data for each track:\",df_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601fe0003c96bd293ef4dab881a52952bfc2e11c"
   },
   "source": [
    "We now proceed to create and train the LightGBM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d565ffb75b5d07695ee52de2830028a396133e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.97131\n",
      "[400]\tvalid_0's auc: 0.973004\n",
      "[600]\tvalid_0's auc: 0.973358\n",
      "Early stopping, best iteration is:\n",
      "[685]\tvalid_0's auc: 0.973458\n",
      "took 5.223278522491455 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "s=time.time()\n",
    "columns=['svolume','nclusters', 'nhitspercluster', 'xmax','ymax','zmax', 'xmin','ymin','zmin', 'zmean', 'xvar','yvar','zvar']\n",
    "rounds=1000\n",
    "round_early_stop=100\n",
    "parameters = { 'subsample_for_bin':800, 'max_bin': 512, 'num_threads':8, \n",
    "               'application': 'binary','objective': 'binary','metric': 'auc','boosting': 'gbdt',\n",
    "               'num_leaves': 128,'feature_fraction': 0.7,'learning_rate': 0.05,'verbose': 0}\n",
    "train_data = lightgbm.Dataset(df_train[columns].values, label=y_train)\n",
    "test_data = lightgbm.Dataset(df_test[columns].values, label=y_test)\n",
    "model = lightgbm.train(parameters,train_data,valid_sets=test_data,num_boost_round=rounds,early_stopping_rounds=round_early_stop,verbose_eval=200)\n",
    "print('took',time.time()-s,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9a7d872c9ae35b65913ec0251262570119ecd6d"
   },
   "source": [
    "### Judge machine learning model\n",
    "\n",
    "We doublecheck the model's performance, by calculating its precision, recall and accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e54c33c409e62e353b07f0ec981ad8cf8294a99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.1  --- Precision: 0.8028, Recall: 0.9903, Accuracy: 0.8693\n",
      "Threshold 0.5  --- Precision: 0.9115, Recall: 0.9259, Accuracy: 0.9152\n",
      "Threshold 0.9  --- Precision: 0.9711, Recall: 0.7144, Accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=model.predict(df_test[columns].values)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.1)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.5)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa41aa28075d5242c5aa1b6e054cfcdf5b83554"
   },
   "source": [
    "### Use machine learning model\n",
    "\n",
    "Merge the two submissions, which were generated using clustering, based on the probabilities of its track candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "db72968ce0c1f3b4ed017ffed2d466196bfdf2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge submission 0 and 1 into sub01:\n",
      "Score: 0.6773030530600002\n"
     ]
    }
   ],
   "source": [
    "preds={}\n",
    "preds[1]=get_predictions(resa1,hits,model)\n",
    "preds[2]=get_predictions(resa2,hits,model)\n",
    "print('Merge submission 0 and 1 into sub01:')\n",
    "sub01=merge_with_probabilities(resa1,resa2,preds[1],preds[2],None,length_factor=0.5)\n",
    "score = score_event_fast(truth, sub01)\n",
    "print('Score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac9082fbb0c411b9e766ad07f6dbfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=7305.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7305/7305 [03:49<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.7543045200999999\n"
     ]
    }
   ],
   "source": [
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "nresa=expand_tracks(sub01,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "nresa['event_id']=0\n",
    "score = score_event_fast(truth, nresa)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Create 4*2250 pairs clustering, merge with ML, and expand at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498c1acc84364dccae4dcc7a35988433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7cd8c8391e49d7bfd182f58216896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d3fbbb6b1e41d999188c9a2cd56e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5811aff0ef24720b62c8ee476300a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fd574d997d498382660ef1dbfe9910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ccf8ce4c494e94886e44458c12179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a1cad7ec504a70b07a419d5e5c99ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250 [01:41<00:00, 22.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 101.53800 sec\n",
      "Your score:  0.57741340359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bfcb6e0e0c4a2c9406292109d2c953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f44cf714dda4f5c8ead0f10d98ef604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a284588bcc40ffa427c150399d0350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6333e5c5df4b4d28855334fda45983b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5144d92683da4bffb9ce672406e50099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28136f1b8a7443dd98735013a9be31a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700585da470b4689ad319a547d18c3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250 [01:41<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 101.67867 sec\n",
      "Your score:  0.5778451737299999\n",
      "Merge submission stage: 1\n",
      "Score: 0.62917734509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1557ce75101e4eef8b76ce2986208d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e2980aeb9344c39a591de94f927ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fdf56cdabc4ebca5f59b7e71498a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70fa73608c84ae4a7a2237d870d53e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e770706906b4ae29f4538ab7cf59ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd44827f4635468cabdaf2562f87c5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efde80a5e6f74ea99d9e23717a5bc212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250 [01:41<00:00, 22.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 101.35383 sec\n",
      "Your score:  0.5785088794900001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdba3bfad73246a19f1766f52bfc14ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124568322687487e8a88ecfb9499c27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8d9e50ab51415b896c0c2381be6558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b225f1ad2ceb471abbf0377009935a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e503cbf3f7f46158f99865d5ac58c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a32a640d06b4108b5be0327cc69a8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496aad513c4341b89792807d502adf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250 [01:41<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 101.40930 sec\n",
      "Your score:  0.57850939805\n",
      "Merge submission stage: 1\n",
      "Score: 0.6286514243800001\n",
      "Merge submission stage: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6753546458300002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc80a7744d348fcbf20a9c0d9db32a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=7186.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7186/7186 [03:47<00:00, 31.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.7530926931899999\n"
     ]
    }
   ],
   "source": [
    "def binary_cluster_merge(stage):\n",
    "    if stage==0:\n",
    "        weights={'pi':1,'theta':0.15}\n",
    "        stds={'z0':7.5, 'kt':7.5e-4}\n",
    "        d =    {'sint':[225,110,110],\n",
    "                'cost':[225,110,110],\n",
    "                  'phi':[550,260,260],\n",
    "                'min_group':[11,11,10],\n",
    "                'npoints':[250,1250,750]}\n",
    "        filters=pd.DataFrame(d)\n",
    "        nu=250\n",
    "        res=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "        res[\"event_id\"]=event_num\n",
    "        score = score_event_fast(truth, res.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "        print(\"Your score: \", score)\n",
    "    else:\n",
    "        res1=binary_cluster_merge(stage-1)\n",
    "        res2=binary_cluster_merge(stage-1)\n",
    "        preds1=get_predictions(res1,hits,model)\n",
    "        preds2=get_predictions(res2,hits,model)\n",
    "        print('Merge submission stage:',stage)\n",
    "        res=merge_with_probabilities(res1,res2,preds1,preds2,None,length_factor=0.5)\n",
    "        score = score_event_fast(truth, res)\n",
    "        print('Score:',score)\n",
    "    return(res)\n",
    "\n",
    "res = binary_cluster_merge(2)\n",
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "res=expand_tracks(res,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "res['event_id']=0\n",
    "score = score_event_fast(truth, res)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
