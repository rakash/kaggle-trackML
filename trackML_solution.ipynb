{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01baac3757b22b92747599ed87524c1ddc0ad1d"
   },
   "source": [
    "# Ultrafast sparse binning clustering enhanced by ML track scoring\n",
    "\n",
    "###  Yuval Reina, Tel-Aviv Israel, [Yuval.reina@gmail.com](Yuval.reina@gmail.com)\n",
    "###  Trian Xylouris, Frankfurt am Main Germany, [t.xylouris@gmail.com](t.xylouris@gmail.com)\n",
    "\n",
    "August 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competition Name: TrackML\n",
    "#### Team Name: Yuval & Trian\n",
    "#### Private Leaderboard Score: 0.80414\n",
    "#### Private Leaderboard Place: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summery\n",
    "\n",
    "This method is using sparse binning to perform ultrafast clustering, tracks are first chosen according to their length, and later are scored and merged using a Machine Learning algorithm. At the final stage the tracks are expanded by adding the closest hits to the track.\n",
    "\n",
    "**The biggest advantage of using clustering by sparse binning is speed**. This method can score 0.5 in just 40 sec using python on a single core, and it could be much faster if it was written in C++ and even faster by using paralleling on CPU or GPU. **The computational complicity of the clustering part in the algorithm is O(N)**. The feature calculation and binning can be done for every hit independent of the other hits and all hits are needed only for counting the number of hits in every bin. (The Python implementation uses np.unique which is actually O(NlogN)  ). We believe that with a careful implementation in C++  a score of 0.5 can be achieved in less than 10 sec which makes this algorithm an essential step in every fast algorithm (we will explain this statement later).\n",
    "\n",
    "The second stage in the algorithm in ML merging. The fastest way to merge 2 solutions is by letting each hit choose the track with the highest number of hits, this method is used in the clustering main loop. This method is limited because the number of hits is not always a good indication for a good track. The ML algorithm is uses many features which describe the track and is able to distinguish between good and bad tracks. Unlike other ML solution presented in Kaggle, our ML algorithm does not check the helix itself, as the clustering part already took care of this.\n",
    "\n",
    "The last part in the algorithm is quite straightforward, track expanding is done by choosing the long tracks and adding to these tracks the closest loose hits (i.e. hits from short tracks). \n",
    "\n",
    "The full solution is then: \n",
    "•\tRun clustering a few times\n",
    "•\tUse ML to merge\n",
    "•\tExpand \n",
    "\n",
    "In the solution we submitted we used about 7 runs of clustering, using 100000 loops each, merged and expanded twice (The 2nd time was just to add about 0.003 to the score by using a loophole in the score definition [(1)](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using Sparse Binning\n",
    "\n",
    "The basic idea of clustering it to relate to every hit a set of numbers (features) and then to group hits with close feature together.\n",
    "The first stage in clustering is finding features which would describe a track and which we can calculate separately for every hit. \n",
    "We have published a post on criteria for good features [(2)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590).\n",
    "\n",
    "### Features \n",
    "Due to the magnetic field in the experiment, the particles form helixes, which travel along the z-axis. Each hit in the x-y-z space can be the member of any particular helix in a certain family of helixes. The idea of the algorithm is to go over each possible member of this family of helixes, and look at how many of all hits may belong to this particular member. The hits are then assigned to the helix (=track) with the most possible hits.\n",
    "\n",
    "We start by assuming the particle is formed in a small cylinder around the origin i.e. the approximate starting point of the helix is '(0,0,z0)'. (According to the introduction papers '|z0|<5.5mm').\n",
    "\n",
    "This kind of helix can be defined by 3 numbers, its Radius, its tangential angle at the origin in the xy plan (=the direction of the particle when it is created, in xy plan), the slop of the helix (how fast the particle moves in the Z direction compared to its velocity in xy).\n",
    "\n",
    "We will define:\n",
    "\n",
    "R – helix radius\n",
    "\n",
    "$\\theta$ – tangential angle in the xy plan\n",
    "\n",
    "$\\phi$ – Slope\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta = \\arctan\\frac{py}{px} \\label{x1} \\\\\n",
    "&\\phi = \\arctan\\frac{pz}{\\sqrt{px^2+py^2}} \\label{x2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where px, py, pz, are the particle’s initial momentum \n",
    "\n",
    "If we take a family of helixes with a radius R, we can calculate for every hit '(x,y,z)' the values for theta and phi.\n",
    "We define\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&kt = \\frac{1}{2R} \\label{x3} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We use kt>0 for particle rotating to the CW and kt<0 for CCW rotation\n",
    "\n",
    "Using sum trigonometry, we will get:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta\\_ = \\arctan\\frac{py}{px}\\label{x4} \\\\\n",
    "\\\\\n",
    "&rr=\\sqrt{x^2+y^2} \\label{x5} \\\\\n",
    "\\\\\n",
    "&\\Delta\\theta=\\arcsin{(kt\\cdot rr)} \\label{x6} \\\\\n",
    "\\\\\n",
    "&\\theta=\\theta\\_+\\Delta\\theta \\label{x7}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "(At this stage we ignore particles that rotate more then $\\pi$ radians)\n",
    "$$ \\begin{align*}\n",
    "&\\phi\\_ = \\arctan\\frac{(z-z0)\\cdot kt}{\\Delta\\theta} \\label{x8}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\theta$ and $\\phi\\_$ will be our features, but we need to tweak them a little to become useful.\n",
    "\n",
    "If $\\theta_1 = \\phi+\\epsilon$ and $\\theta_1 = \\phi-\\epsilon$,  $\\theta_1 – \\theta_2= 2\\epsilon$ on a circle, but if we calculate $\\theta_1 – \\theta_2$ we will get $2\\pi+2\\epsilon$. To solve this issue we will use 2 features instead:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&sint = \\sin(\\theta) \\label{x9}\\\\\n",
    "&cost = \\cos(\\theta) \\label{x10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\phi\\_$ on the other hand is in the range $[-\\pi/2,\\pi/2]$, and does not have the above issue, but $\\phi\\_$ distribution is far from being uniform (for an extensive discussion about its distribution look here [(3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-371940)). \n",
    "\n",
    "The solution we found suitable is by defining:\n",
    "$$ \\begin{align*}\n",
    "&\\phi = \\arctan\\frac{(z-z0)\\cdot kt}{3.3\\cdot\\Delta\\theta}\\frac{2}{\\pi} \\label{x11}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(the last edition is for scaling to a [-1,1] range)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Binning Clustering\n",
    "Binning is the easiest way to cluster, in this method we divide the values of each feature to constant areas and a cluster is formed by all the hits whos features are all in the same area. \n",
    "\n",
    "As an example, if we have one feature in the range `[-1,1]` we can decide to divide it to 20 equal bins of size `0.1: [-1,-0.9), [-0.9,0.8), …,[0.9,1]`. We will assign every bin a number 0 , …, 19 and define the hit’s feature with this value. If we have 3 features for a hit we will define that hit with the 3 values of bins. As an example:\n",
    "\n",
    "`\n",
    "Hit #1 – (12,15,0)\n",
    "Hit #2 – (11,5,2)`\n",
    "\n",
    "Two hits which have the exact 3 values, are clustered together to form track candidate.\n",
    "\n",
    "In general, the bins don’t have to spread uniformly, and every feature can have a different number of bins.\n",
    "\n",
    "In Sparse Binning we use a very simple binning algorithm. We start by using uniform spread bins which can be easily calculated:\n",
    "\n",
    "For feature `F1` in the range `[-1,1]` we define `B1 = int(F1*k1)` where the number of bins is `2*k1`\n",
    "\n",
    "i.e. we multiply the feature by half the number of bins and take the integer part.\n",
    "\n",
    "To combine few features together and get the cluster id (track_id) we just offset each feature by multiplying it by a number which is big enough. \n",
    "\n",
    "If we have 2 features F1, F2 and we want to have 2*k1, 2*k2 bins for each feature, we can define:\n",
    "\n",
    "` (a) Track_id = int(F1*k1) + (2*k1+1)*int(F2*k2)`\n",
    "\n",
    "And that’s it clustering is finished!!\n",
    "\n",
    "This calculation takes about 4mSec for 120,000 hits on Kaggle kernel platform compared to about 950mSec for dbscan which is the most popular clustering algorithm used by the competitors in the TrackML Challenge. \n",
    "\n",
    "This type of clustering moves the bottleneck of the full solution, form clustering to merging (will be discussed in the next section).\n",
    "\n",
    "One weakness of the binning algorithm is its sensitivity caused, among other things, by the hard boarders between bins. As an example, let’s take two hits:\n",
    "\n",
    "`\n",
    "Hit1 – with features (-1e-7,0.1,0.3)\n",
    "Hit2 – with features (1e-7,0.1,0.3)`\n",
    "\n",
    "If we use the calculation above (1), Hit1, Hit2 will never cluster together no matter how many bins we’ll have.\n",
    "\n",
    "This issue is solved by adding a random number in the range `[0,1)` to every feature in the binning calculation, making (a):\n",
    "\n",
    "`(b) Track_id = int(F1*k1+r1) + (2*k1+1)*int(F2*k2+r2)`\n",
    "\n",
    "Where r1, r2 are random numbers with uniform distribution in the range `[0,1)`. r1, r2 are changed every time we cluster.\n",
    "\n",
    "When compared to dbscan, sparse clustering has one big disadvantage, it’s sensitivity, it can miss some of the hits because they fall in another bin. This sensitivity is also its strength, as it doesn’t cluster many wrong hits (outliners) which degrade the maximum achievable score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Merging\n",
    "Every time we cluster with a set of kt, z0, every hit gets a new track_id, to have a full solution we want to merge two lists of track id’s together. The most efficient way we found was to measure the length of each track in the two lists, and let every hit choose the longest track it can.\n",
    "\n",
    "If Hit #1 has track_id=1 in list #100 and track_id=10000002 in list #101 (the track_id’s in both lists should be completely different) and in list #100 there are 10 hits with track_id=1 while in list #101 there are 7 hits with track_id=10000002, hit#1 will choose track_id=1.\n",
    "\n",
    "To do this, we need to measure the track’s length. The fastest way we know to do this in Python is by using numpy.unique. This operation takes 11mSec for 120000 hits on Kaggle kernel platform. The numpy.unique computational complexity is O(NlogN) because it sorts the values. While if we have enough memory, this task can easily be done in O(N). (We can also reduce the amount of memory needed by hashing). \n",
    "\n",
    "In our main loop we measure the length of a track twice, the first time we do as we described above to create a track candidate and then we measure again to make sure the track is still long enough as hits may decide to choose another track in the first round. \n",
    "\n",
    "The double track length measurement makes this the bottleneck of the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering – the full algorithm\n",
    "The algorithm iterate over random selected pairs – (z0,kt). For every pair the hits’ features are calculated, and clustering is performed by the sparse binning algorithm. The new clustering result is then merged with previous ones with the merging algorithm described above. \n",
    "\n",
    "Every 500 loops an extra step is taken. Tracks are examined for hits coming from the same detector (equal Volume_id, Layer_id, and Module_id) or from the same layer. There can’t be two hits from the same detector or 3 from the same layer. The extra hits, are discovered by measuring the distance between the hit and the center of gravity of the track. These hits are removed from the track. \n",
    "\n",
    "In this step the algorithm also permanently set tracks which are long enough (hits won’t we taken away or added to this track any more).\n",
    "\n",
    "While running the algorithm can change the number of bins per feature and the length of the minimal track to set, these settings are user defined parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering results:\n",
    "The score common score after the clustering algorithm with as a function of  the number of (z0,kt) pairs used is presented in the following table (measured on event 000001000 from the training set – usually 0.015 below final LB score):\n",
    "\n",
    "|   Pairs   |  Score |\n",
    "|-----------|--------|\n",
    "|    1000\t|  0.51  |\n",
    "|    1600\t|  0.56  |\n",
    "|    5500\t|  0.636 |\n",
    "|  100000\t|  0.73  |\n",
    "\n",
    "The score plateau after about 90000 pairs and wouldn’t get higher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Machine Learning\n",
    "We run the binning algorithm 8 times in total, and merge the resulting solutions, using a machine algorithm model. In particular, we use LightGBM, but expect that a deep neural network or even Logistic Regression would yield similar results. The machine learning algorithm predicts the probabilitiy that a track is good, based on what it has learned from the training events. In particular, it looks at 13 features for each track. This improvement increases the score from ~0.73 to ~0.76. \n",
    "\n",
    "Note: if we increased the number of merged solutions to 30, we expect to get ~0.78 instead of ~0.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Track Extension\n",
    "The major drawback of the binning clustering algorithm is its sensitivity. The bins sharp boarders can leave a hit out of the track although it is very close to the other hits. To overcome this issue, a track expanding algorithm is the final step in the full solution.\n",
    "\n",
    "The final clustering and merging solution it taken as a basis for this step.\n",
    "\n",
    "First, the algorithm tries to improve the (z0,kt) pair for each track. It does this by searching for a pair which will minimize the standard deviation of the track hits.\n",
    "\n",
    " Using these refined (kt,z0) values. The hits which are closest to track are added to it. The distance between a hit and a track is measured by the difference between the its calculated features and the mean of the tracks features.\n",
    " \n",
    "We also tested a variant of this algorithm which measure the minimal distance to one of the track’s hit (as in nearest neighbor algorithm), this variant performed slightly better but was much slower.  \n",
    "\n",
    "The improvement gained by expanding depends on the score after the previous stages.\n",
    "\n",
    "0.63 score can jump to 0.73,\n",
    "\n",
    "0.73 would go to 0.79\n",
    "\n",
    "and in our final run 0.76 was improved to 0.804\n",
    "\n",
    "This input to this stage must come from good clustering algorithm and cascaded extending will usually degrade the score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "\n",
    "1.\tAs a full solution our approach can be easily optimized for speed:\n",
    "    -\tThe clustering algorithm can be improved as described above. Farther improvement for real world application can be achieved by massively paralleling the feature calculation using a GPU (as we explained above, every hit is treated separately, which makes the algorithm very suitable for parallel processing).\n",
    "    -\tBoth the ML and the expanding algorithm where not optimized at all as they weren’t the bottleneck of the solution. As an example, the expanding algorithm recalculates the features for all the loose hits every time. Calculating the features only for hits with z>0 while expanding tracks with z>0 will immediately cut the running time of the algorithm by half. \n",
    "2.\tElements of our approach may be beneficial for any final solution to this problem.\n",
    "   - In particular, our binning algorithm creates a lot of good candidate tracks in a short period of time. It may be helpful to employ this at the beginning of a solution, in order to very quickly (<1 minute) detect 50% of all tracks (use small bins). One can then continue with running a different algorithm on the remaining hits.\n",
    "   - Similarly, the machine learning algorithm, as well as the employed parameters and track extension algorithm may improve any final result, while just adding minutes (or less, after optimization) to the total runtime.\n",
    "3.\tIn our algorithm we didn’t do any adjustments for uneven magnetic field. If we incorporate @CPMP’s findings [here (3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564). The score of the algorithm improves immediately by 0.02 to about 0.82. \n",
    "4.\tAn obvious short-coming of   main approach is, that it is not suited to find tracks, which originate far from the origin. We did try to adjust our algorithm to work also in that situation, but, had minimal success so far.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "(1)  @Grzegorz Sionkowski comment in *\"how to score a track is good or not\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)\n",
    "\n",
    "(2) *\"Criteria for good features\"* [(https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)\n",
    "\n",
    "(3) @CPMP *\"Solution #9\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Running full pipeline for train event 1000\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We will demonstrate how to get a score of TBD for train event 1000, in TBD minutes, by using our aforementioned method. The steps are: \n",
    "- Create 2 initial solutions: 2x use 5.500 pairs of (kt,z0) for binning (in our original solution, we use 100.000 pairs)\n",
    "- Merge the 2 solutions using a machine learning algorithm (in our original solution, we use 8)\n",
    "- Extend the tracks (in our original solution, we run this twice, while at the first time, we extend hits according to whether their values for (1) and (2) are close enough to at least 1 hit, instead of the mean of the track)\n",
    "\n",
    "Import necessary packages and load train event 1000, which will be the event we will be working on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'other/')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from ipywidgets import FloatProgress,FloatText\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import gc\n",
    "import cProfile\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)})\n",
    "plt.figure(figsize=(12,5))\n",
    "path='files/'\n",
    "\n",
    "from functions.other import calc_features, get_event, score_event_fast, load_obj\n",
    "from functions.expand import *\n",
    "from functions.cluster import *\n",
    "from functions.ml_model import merge_with_probabilities,precision_and_recall,get_features,get_predictions\n",
    "\n",
    "# the following two lines are for changing imported functions, and not needing to restart kernel to use their updated version\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "event_num=0\n",
    "event_prefix = 'event00000100{}'.format(event_num)\n",
    "hits, cells, particles, truth = get_event(path,event_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Define parameters and run clustering, twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "00cff1de64d7a7f24df2e9430344f5ae90dc953c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3db6239bd449db9b81e73e40296a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655bf49194943009b9fc6071c1f3953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dfd57064474cc6a8080a1cfe4ba529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9703bfaea41400b8f56daa180a1a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e871bace22e4fb99285507f152688dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a8ab44de6d49df8938fb54ee7f929d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ce8dd8fd040cbac2d55f92c8bac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 37.83613 sec\n",
      "Your score:  0.50083903754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28b6bd0d9d8442294c731669071388e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8204930a27e433baf9484158ea3d677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f0fed14e1641d69ef9b39e5e927128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fef3f85b2da403aa9ba12a16b5d73fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab4f1328bf04698ab102ddbe786ea90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac4fdda8b124fe485e76314e24b7547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84231cf1dfc4821b867e5f80c48281d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 38.06680 sec\n",
      "Your score:  0.49943215791999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "weights={'pi':1,'theta':0.15}\n",
    "stds={'z0':7.5, 'kt':7.5e-4}\n",
    "d =    {'sint':[225,110,110,110,110,110],\n",
    "        'cost':[225,110,110,110,110,110],\n",
    "          'phi':[550,260,260,260,260,260],\n",
    "        'min_group':[11,11,10,9,8,7],\n",
    "        'npoints':[200,200,200,200,100,100]}  # quick run for testing\n",
    "        #'npoints':[500,2000,1000,1000,500,500]}\n",
    "filters=pd.DataFrame(d)\n",
    "#nu=500\n",
    "nu=100\n",
    "\n",
    "resa1=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa1[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa1.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)\n",
    "\n",
    "resa2=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa2[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa2.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317879ec4824598fad62b7faba8f981eed065681"
   },
   "source": [
    "## Employ Machine Learning\n",
    "### General strategy\n",
    "- Produce different submission candidates sub_1, sub_2, ..., sub_N\n",
    "- Create a machine learning model, which gives probabilities between 0 and 1 for each track candidate\n",
    "- Merge two submission candidates by assigning to each hit the track, which has higher probability\n",
    "- Merge the submission candidates successively (sub_1 and sub_2 to sub_12, sub_12 and sub_3 to sub_123, etc.) to get the final submission\n",
    "\n",
    "[Note: For our final solution, the methods described in this chapter gave around +0.01 to the LB score. Certain benefits from these methods have already been captured by the function, which expands the tracks.]\n",
    "\n",
    "### Create machine learning model (LightGBM)\n",
    "- Training data: Use the truth file of the train events 3-5 to get true tracks (target=1). To get wrong tracks (target=0) use generated clustering submissions on those latter events. In particular, we consider each track candidate, for which not all hits belong to the same particle_id, as a wrong track. Also, choose only tracks which have at least length 4, to slightly optimize compute time at negligible cost.\n",
    "- Use 13 features per track: \n",
    " - variance of x,y,z (these are the most important)\n",
    " - minimum of x,y,z\n",
    " - maximum of x,y,z\n",
    " - mean of z\n",
    " - volume_id of first hit \n",
    " - number of clusters per track (i.e. are there many hits, which are close together)\n",
    " - number of hits / number of clusters   \n",
    "- Validation data: Same as with training data, but use the 3 training events 0,1,2\n",
    "\n",
    "[Note: We also tried many more features (compare below dataframe df_train), but they gave only small additional gains, so we just kept those 13 features for simplicity. In fact, many other interesting features, such as number of hits, number of different volumes crossed etc are closely related to the above used ones.]\n",
    "\n",
    "We have prepared the training and test data and load it directly from a pkl-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "5800822ec8d5798c6bc9ecba2df5f20964fe13cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with all features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cells_length</th>\n",
       "      <th>cells_max</th>\n",
       "      <th>cells_mean</th>\n",
       "      <th>cells_min</th>\n",
       "      <th>cells_var</th>\n",
       "      <th>elayer</th>\n",
       "      <th>emodule</th>\n",
       "      <th>evolume</th>\n",
       "      <th>ex</th>\n",
       "      <th>ey</th>\n",
       "      <th>...</th>\n",
       "      <th>ymean</th>\n",
       "      <th>ymedian</th>\n",
       "      <th>ymin</th>\n",
       "      <th>yvar</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zmedian</th>\n",
       "      <th>zmin</th>\n",
       "      <th>zvar</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.341868</td>\n",
       "      <td>0.119788</td>\n",
       "      <td>2.044216e-02</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-290.533997</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>...</td>\n",
       "      <td>-256.535034</td>\n",
       "      <td>-286.803009</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>26005.789062</td>\n",
       "      <td>1801.500000</td>\n",
       "      <td>951.502136</td>\n",
       "      <td>1031.050049</td>\n",
       "      <td>107.703003</td>\n",
       "      <td>3.882222e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.450497</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>2.354972e-14</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-231.250000</td>\n",
       "      <td>351.147003</td>\n",
       "      <td>...</td>\n",
       "      <td>143.303238</td>\n",
       "      <td>92.862198</td>\n",
       "      <td>20.085199</td>\n",
       "      <td>10862.027344</td>\n",
       "      <td>-244.580994</td>\n",
       "      <td>-1334.507324</td>\n",
       "      <td>-958.000000</td>\n",
       "      <td>-2951.500000</td>\n",
       "      <td>7.111888e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.680366</td>\n",
       "      <td>0.167157</td>\n",
       "      <td>1.636995e-02</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>470.838989</td>\n",
       "      <td>768.648010</td>\n",
       "      <td>...</td>\n",
       "      <td>378.134766</td>\n",
       "      <td>309.616516</td>\n",
       "      <td>30.304199</td>\n",
       "      <td>72788.718750</td>\n",
       "      <td>1802.500000</td>\n",
       "      <td>782.384949</td>\n",
       "      <td>561.099976</td>\n",
       "      <td>64.440201</td>\n",
       "      <td>3.828446e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.142238</td>\n",
       "      <td>0.461141</td>\n",
       "      <td>1.039750e-01</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.234398</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>...</td>\n",
       "      <td>-450.889038</td>\n",
       "      <td>-434.042480</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>130411.406250</td>\n",
       "      <td>2947.500000</td>\n",
       "      <td>1532.954224</td>\n",
       "      <td>1469.185059</td>\n",
       "      <td>245.947006</td>\n",
       "      <td>1.497747e+06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.791248</td>\n",
       "      <td>0.182531</td>\n",
       "      <td>3.750272e-02</td>\n",
       "      <td>0.052045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>135.772003</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>...</td>\n",
       "      <td>-389.764557</td>\n",
       "      <td>-330.183014</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>91312.648438</td>\n",
       "      <td>-71.102097</td>\n",
       "      <td>-869.824463</td>\n",
       "      <td>-752.000000</td>\n",
       "      <td>-2152.500000</td>\n",
       "      <td>4.269275e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cells_length  cells_max  cells_mean     cells_min  cells_var  elayer  \\\n",
       "0           4.0   0.341868    0.119788  2.044216e-02   0.017071     6.0   \n",
       "1           8.0   0.450497    0.071357  2.354972e-14   0.020859     2.0   \n",
       "2          11.0   0.680366    0.167157  1.636995e-02   0.035665     6.0   \n",
       "3           3.0   1.142238    0.461141  1.039750e-01   0.232131    12.0   \n",
       "4          10.0   0.791248    0.182531  3.750272e-02   0.052045     6.0   \n",
       "\n",
       "   emodule  evolume          ex          ey   ...          ymean     ymedian  \\\n",
       "0     29.0     14.0 -290.533997 -463.497009   ...    -256.535034 -286.803009   \n",
       "1    145.0     12.0 -231.250000  351.147003   ...     143.303238   92.862198   \n",
       "2     66.0     18.0  470.838989  768.648010   ...     378.134766  309.616516   \n",
       "3     26.0     18.0   94.234398 -865.289001   ...    -450.889038 -434.042480   \n",
       "4     27.0     16.0  135.772003 -984.747986   ...    -389.764557 -330.183014   \n",
       "\n",
       "         ymin           yvar         zmax        zmean      zmedian  \\\n",
       "0 -463.497009   26005.789062  1801.500000   951.502136  1031.050049   \n",
       "1   20.085199   10862.027344  -244.580994 -1334.507324  -958.000000   \n",
       "2   30.304199   72788.718750  1802.500000   782.384949   561.099976   \n",
       "3 -865.289001  130411.406250  2947.500000  1532.954224  1469.185059   \n",
       "4 -984.747986   91312.648438   -71.102097  -869.824463  -752.000000   \n",
       "\n",
       "          zmin          zvar event_id  \n",
       "0   107.703003  3.882222e+05        3  \n",
       "1 -2951.500000  7.111888e+05        3  \n",
       "2    64.440201  3.828446e+05        3  \n",
       "3   245.947006  1.497747e+06        3  \n",
       "4 -2152.500000  4.269275e+05        3  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for each track: ['cells_length' 'cells_max' 'cells_mean' 'cells_min' 'cells_var' 'elayer'\n",
      " 'emodule' 'evolume' 'ex' 'ey' 'ez' 'hitids' 'nclusters' 'nhits'\n",
      " 'nhitspercluster' 'nsensors' 'nvolumes' 'particle_id' 'sentence'\n",
      " 'sentence_count' 'sentence_value' 'slayer' 'smodule' 'svolume' 'sx' 'sy'\n",
      " 'sz' 'target' 'xmax' 'xmean' 'xmedian' 'xmin' 'xvar' 'ymax' 'ymean'\n",
      " 'ymedian' 'ymin' 'yvar' 'zmax' 'zmean' 'zmedian' 'zmin' 'zvar' 'event_id']\n"
     ]
    }
   ],
   "source": [
    "df_train=load_obj('files/df_train_v2-reduced.pkl')\n",
    "df_test=load_obj('files/df_test_v1.pkl')\n",
    "y_train=df_train.target.values\n",
    "y_test=df_test.target.values\n",
    "print(\"The dataframe with all features:\")\n",
    "display(df_train.head())\n",
    "print(\"Features for each track:\",df_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601fe0003c96bd293ef4dab881a52952bfc2e11c"
   },
   "source": [
    "In the competition, we used roughly 250 events for training, but the additional improvement to just using 13 events is not too big. We now create the LightGBM model, using the mentioned training data and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "d565ffb75b5d07695ee52de2830028a396133e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.954964\n",
      "[100]\tvalid_0's auc: 0.960705\n",
      "[150]\tvalid_0's auc: 0.962809\n",
      "[200]\tvalid_0's auc: 0.963605\n",
      "[250]\tvalid_0's auc: 0.964108\n",
      "[300]\tvalid_0's auc: 0.964196\n",
      "[350]\tvalid_0's auc: 0.964214\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's auc: 0.964268\n",
      "took 1.4954051971435547 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "s=time.time()\n",
    "# choose which features of the tracks we want to use:\n",
    "columns=['svolume','nclusters', 'nhitspercluster', 'xmax','ymax','zmax', 'xmin','ymin','zmin', 'zmean', 'xvar','yvar','zvar']\n",
    "rounds=1000\n",
    "round_early_stop=50\n",
    "parameters = { 'subsample_for_bin':800, 'max_bin': 512, 'num_threads':8, \n",
    "               'application': 'binary','objective': 'binary','metric': 'auc','boosting': 'gbdt',\n",
    "               'num_leaves': 128,'feature_fraction': 0.7,'learning_rate': 0.05,'verbose': 0}\n",
    "train_data = lightgbm.Dataset(df_train[columns].values, label=y_train)\n",
    "test_data = lightgbm.Dataset(df_test[columns].values, label=y_test)\n",
    "model = lightgbm.train(parameters,train_data,valid_sets=test_data,num_boost_round=rounds,early_stopping_rounds=round_early_stop,verbose_eval=50)\n",
    "print('took',time.time()-s,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9a7d872c9ae35b65913ec0251262570119ecd6d"
   },
   "source": [
    "### Judge machine learning model\n",
    "\n",
    "We doublecheck the model's performance, by calculating its precision, recall and accuracy on the validation set:\n",
    "\n",
    "[Note: For ML to be helpful in our situation, it needs to distinguish correct from wrong tracks with very high precision=true_positives/(false_positives+true_positives)). Also, it needs to do so for various sets of track candidates (especially such, which are generated if one tries to find tracks which originate far away from the origin; in those situations, often a lot of bad candidates are produced).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "e54c33c409e62e353b07f0ec981ad8cf8294a99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.1  --- Precision: 0.8458, Recall: 0.9955, Accuracy: 0.8861\n",
      "Threshold 0.5  --- Precision: 0.9060, Recall: 0.9608, Accuracy: 0.9150\n",
      "Threshold 0.9  --- Precision: 0.9621, Recall: 0.7437, Accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=model.predict(df_test[columns].values)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.1)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.5)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa41aa28075d5242c5aa1b6e054cfcdf5b83554"
   },
   "source": [
    "### Use machine learning model\n",
    "\n",
    "Calculate the probabilities for the tracks in those two submissions (small optimization: take also length of track into account, and after a couple of merged submissions, ask the probability of the track from the new submission to be at least C higher than the current probability; this latter option is not used in this kernel, but was used when merging >= 4 submissions). \n",
    "\n",
    "Then merge both submissions, based on the probabilities of its track candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "db72968ce0c1f3b4ed017ffed2d466196bfdf2f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfe897c53d642c3a01c995cd64902e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33927), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd1842d22ff45db960c777068539b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33756), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge submission 0 and 1 into sub01:\n",
      "Score: 0.5651194312399999\n"
     ]
    }
   ],
   "source": [
    "preds={}\n",
    "preds[1]=get_predictions(resa1,hits,model)\n",
    "preds[2]=get_predictions(resa2,hits,model)\n",
    "print('Merge submission 0 and 1 into sub01:')\n",
    "sub01=merge_with_probabilities(resa1,resa2,preds[1],preds[2],None,length_factor=0.5)\n",
    "score = score_event_fast(truth, sub01)\n",
    "print('Score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 14.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f4a4cb207b4e8184be51386f93ad14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=6763.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6763/6763 [02:29<00:00, 45.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.6942199117400001\n"
     ]
    }
   ],
   "source": [
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "nresa=expand_tracks(sub01,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "nresa['event_id']=0\n",
    "score = score_event_fast(truth, nresa)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
