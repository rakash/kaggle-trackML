{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01baac3757b22b92747599ed87524c1ddc0ad1d"
   },
   "source": [
    "# Solution #7 of Kaggle TrackML competition\n",
    "\n",
    "by Yuval Reina, Trian Xylouris\n",
    "\n",
    "in 2018\n",
    "\n",
    "\n",
    "## Chapter 1: Methods and Results\n",
    "\n",
    "The current paper demonstrates our solution to Kaggle's TrackML competition in 2018, which yielded a private leaderboard score of 0.8041 and place 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our basic method: A variant of the Hough transform\n",
    "\n",
    "Due to the magnetic field in the experiment, the particles form helixes, which travel along the z-axis. Each hit in the x-y-z space can be the member of any particular helix in a certain family of helixes. The idea of the Hough transform is to go over each possible member of this family of helixes, and look at how many of all hits may belong to this particular member. The hits are then assigned to the helix (=track) with the most possible hits.\n",
    "\n",
    "We employed this concept as follows. For each point (x,y,z) on a specific track with radius 1/(2*kt), which originates from (0,0,z0), the following two quantities will be roughly constant:\n",
    "\n",
    "(1) Theta = arctan(y/x)+arcsin(kt*rr)\n",
    "\n",
    "(2)\tGamma = (z-z0)*kt/dtheta\n",
    "\n",
    "where rr=(x^2+y^2)^0.5. It remains to iterate over the radius and z0, store all Theta and Gamma for every hit in the x-y-z space, and put those hits together, whose Theta and Gamma are near each other.\n",
    "\n",
    "To do the latter, we employ several techniques, which increase the speed and accuracy of our solution:\n",
    "- use binning, i.e. divide the 3d space into many bins\n",
    "- every 500 loops all hits belonging to tracks which are long enough are removed from the dataset; also the sizes of the bins are adjusted\n",
    "- iterate over random pairs (kt,z0), where kt and z0 are taken from a normal 1d random distribution with means 7.5e-4 and 7.5\n",
    "- adjust the size and location of each bin with a small random variation\n",
    "- smooth out the values of (1) of (2) using sin/cos and arctan, respectively\n",
    "- if two hits are on the same track and on the same sensor (= same combination of volume, layer, module), we remove the one hit of the two, whose parameters (1) and (2) are further away from the track's mean values of those parameters\n",
    "\n",
    "The disadvantage of sparse binning over dbscan is itâ€™s sensitivity, the advantages are its speed and its precision (almost no outliers). Our basic method, run once with 100.000 pairs, yields a score of ~0.73."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional improvement 1: Machine Learning\n",
    "We run the binning algorithm 8 times in total, and merge the resulting solutions, using a machine algorithm model. In particular, we use LightGBM, but expect that a deep neural network or even Logistic Regression would yield similar results. The machine learning algorithm predicts the probabilitiy that a track is good, based on what it has learned from the training events. In particular, it looks at 13 features for each track. This improvement increases the score from ~0.73 to ~0.76. \n",
    "\n",
    "Note: if we increased the number of merged solutions to 30, we expect to get ~0.78 instead of ~0.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional improvement 2: Track Extension\n",
    "We take the merged solution of improvement 1, and recalculate the (kt,z0) for every single track, such that the parameters (1) and (2) among the track members have minimal standard deviation. Using these refined (kt,z0) values, we extend every track, by assigning those hits onto it, such that parameters (1) and (2), if calculated with the refined (kt,z0), are very close to the track's mean values (or just close enough to at least one hit of the track). \n",
    "\n",
    "We run this operation also a second time, where we specifically look to re-assign hits, which have been assigned until then only to tracks of length 1 or 2.\n",
    "\n",
    "This improvement increased the score from ~0.76 to 0.8041. If we hadn't used improvement 1, then this method improves the score from ~0.73 to ~0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "Elements of our approach may be beneficial for any final solution to this problem. \n",
    "\n",
    "In particular, our binning algorithm creates a lot of good candidate tracks in a short period of time. It may be helpful to employ this at the beginning of a solution, in order to very quickly (<1 minute) detect 50% of all tracks (use small bins). One can then continue with running a different algorithm on the remaining hits.\n",
    "\n",
    "Similarly, the machine learning algorithm, as well as the employed parameters and track extension algorithm may improve any final result, while just adding minutes (or less, after optimization) to the total runtime.\n",
    "\n",
    "An obvious short-coming of our main approach is, that it is not suited to find tracks, which originate far from the origin. We did try to adjust our algorithm to work also in that situation, but had no success so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Running full pipeline for train event 1000\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We will demonstrate how to get a score of TBD for train event 1000, in TBD minutes, by using our aforementioned method. The steps are: \n",
    "- Create 2 initial solutions: 2x use 5.500 pairs of (kt,z0) for binning (in our original solution, we use 100.000 pairs)\n",
    "- Merge the 2 solutions using a machine learning algorithm (in our original solution, we use 8)\n",
    "- Extend the tracks (in our original solution, we run this twice, while at the first time, we extend hits according to whether their values for (1) and (2) are close enough to at least 1 hit, instead of the mean of the track)\n",
    "\n",
    "Import necessary packages and load train event 1000, which will be the event we will be working on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'other/')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from ipywidgets import FloatProgress,FloatText\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from itertools import product\n",
    "import gc\n",
    "import cProfile\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)})\n",
    "plt.figure(figsize=(12,5))\n",
    "path='files/'\n",
    "\n",
    "from functions.other import calc_features, get_event, score_event_fast, load_obj\n",
    "from functions.expand import *\n",
    "from functions.cluster import *\n",
    "from functions.ml_model import merge_with_probabilities,precision_and_recall,get_features,get_predictions\n",
    "\n",
    "# the following two lines are for changing imported functions, and not needing to restart kernel to use their updated version\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "event_num=0\n",
    "event_prefix = 'event00000100{}'.format(event_num)\n",
    "hits, cells, particles, truth = get_event(path,event_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Define parameters and run clustering, twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "00cff1de64d7a7f24df2e9430344f5ae90dc953c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3db6239bd449db9b81e73e40296a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655bf49194943009b9fc6071c1f3953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dfd57064474cc6a8080a1cfe4ba529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9703bfaea41400b8f56daa180a1a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e871bace22e4fb99285507f152688dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a8ab44de6d49df8938fb54ee7f929d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ce8dd8fd040cbac2d55f92c8bac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:37<00:00, 26.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 37.83613 sec\n",
      "Your score:  0.50083903754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28b6bd0d9d8442294c731669071388e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8204930a27e433baf9484158ea3d677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f0fed14e1641d69ef9b39e5e927128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fef3f85b2da403aa9ba12a16b5d73fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab4f1328bf04698ab102ddbe786ea90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac4fdda8b124fe485e76314e24b7547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84231cf1dfc4821b867e5f80c48281d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:37<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 38.06680 sec\n",
      "Your score:  0.49943215791999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "weights={'pi':1,'theta':0.15}\n",
    "stds={'z0':7.5, 'kt':7.5e-4}\n",
    "d =    {'sint':[225,110,110,110,110,110],\n",
    "        'cost':[225,110,110,110,110,110],\n",
    "          'phi':[550,260,260,260,260,260],\n",
    "        'min_group':[11,11,10,9,8,7],\n",
    "        'npoints':[200,200,200,200,100,100]}  # quick run for testing\n",
    "        #'npoints':[500,2000,1000,1000,500,500]}\n",
    "filters=pd.DataFrame(d)\n",
    "#nu=500\n",
    "nu=100\n",
    "\n",
    "resa1=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa1[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa1.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)\n",
    "\n",
    "resa2=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa2[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa2.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317879ec4824598fad62b7faba8f981eed065681"
   },
   "source": [
    "## Employ Machine Learning\n",
    "### General strategy\n",
    "- Produce different submission candidates sub_1, sub_2, ..., sub_N\n",
    "- Create a machine learning model, which gives probabilities between 0 and 1 for each track candidate\n",
    "- Merge two submission candidates by assigning to each hit the track, which has higher probability\n",
    "- Merge the submission candidates successively (sub_1 and sub_2 to sub_12, sub_12 and sub_3 to sub_123, etc.) to get the final submission\n",
    "\n",
    "[Note: For our final solution, the methods described in this chapter gave around +0.01 to the LB score. Certain benefits from these methods have already been captured by the function, which expands the tracks.]\n",
    "\n",
    "### Create machine learning model (LightGBM)\n",
    "- Training data: Use the truth file of the train events 3-5 to get true tracks (target=1). To get wrong tracks (target=0) use generated clustering submissions on those latter events. In particular, we consider each track candidate, for which not all hits belong to the same particle_id, as a wrong track. Also, choose only tracks which have at least length 4, to slightly optimize compute time at negligible cost.\n",
    "- Use 13 features per track: \n",
    " - variance of x,y,z (these are the most important)\n",
    " - minimum of x,y,z\n",
    " - maximum of x,y,z\n",
    " - mean of z\n",
    " - volume_id of first hit \n",
    " - number of clusters per track (i.e. are there many hits, which are close together)\n",
    " - number of hits / number of clusters   \n",
    "- Validation data: Same as with training data, but use the 3 training events 0,1,2\n",
    "\n",
    "[Note: We also tried many more features (compare below dataframe df_train), but they gave only small additional gains, so we just kept those 13 features for simplicity. In fact, many other interesting features, such as number of hits, number of different volumes crossed etc are closely related to the above used ones.]\n",
    "\n",
    "We have prepared the training and test data and load it directly from a pkl-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "5800822ec8d5798c6bc9ecba2df5f20964fe13cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with all features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cells_length</th>\n",
       "      <th>cells_max</th>\n",
       "      <th>cells_mean</th>\n",
       "      <th>cells_min</th>\n",
       "      <th>cells_var</th>\n",
       "      <th>elayer</th>\n",
       "      <th>emodule</th>\n",
       "      <th>evolume</th>\n",
       "      <th>ex</th>\n",
       "      <th>ey</th>\n",
       "      <th>...</th>\n",
       "      <th>ymean</th>\n",
       "      <th>ymedian</th>\n",
       "      <th>ymin</th>\n",
       "      <th>yvar</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zmedian</th>\n",
       "      <th>zmin</th>\n",
       "      <th>zvar</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.341868</td>\n",
       "      <td>0.119788</td>\n",
       "      <td>2.044216e-02</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-290.533997</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>...</td>\n",
       "      <td>-256.535034</td>\n",
       "      <td>-286.803009</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>26005.789062</td>\n",
       "      <td>1801.500000</td>\n",
       "      <td>951.502136</td>\n",
       "      <td>1031.050049</td>\n",
       "      <td>107.703003</td>\n",
       "      <td>3.882222e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.450497</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>2.354972e-14</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-231.250000</td>\n",
       "      <td>351.147003</td>\n",
       "      <td>...</td>\n",
       "      <td>143.303238</td>\n",
       "      <td>92.862198</td>\n",
       "      <td>20.085199</td>\n",
       "      <td>10862.027344</td>\n",
       "      <td>-244.580994</td>\n",
       "      <td>-1334.507324</td>\n",
       "      <td>-958.000000</td>\n",
       "      <td>-2951.500000</td>\n",
       "      <td>7.111888e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.680366</td>\n",
       "      <td>0.167157</td>\n",
       "      <td>1.636995e-02</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>470.838989</td>\n",
       "      <td>768.648010</td>\n",
       "      <td>...</td>\n",
       "      <td>378.134766</td>\n",
       "      <td>309.616516</td>\n",
       "      <td>30.304199</td>\n",
       "      <td>72788.718750</td>\n",
       "      <td>1802.500000</td>\n",
       "      <td>782.384949</td>\n",
       "      <td>561.099976</td>\n",
       "      <td>64.440201</td>\n",
       "      <td>3.828446e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.142238</td>\n",
       "      <td>0.461141</td>\n",
       "      <td>1.039750e-01</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.234398</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>...</td>\n",
       "      <td>-450.889038</td>\n",
       "      <td>-434.042480</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>130411.406250</td>\n",
       "      <td>2947.500000</td>\n",
       "      <td>1532.954224</td>\n",
       "      <td>1469.185059</td>\n",
       "      <td>245.947006</td>\n",
       "      <td>1.497747e+06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.791248</td>\n",
       "      <td>0.182531</td>\n",
       "      <td>3.750272e-02</td>\n",
       "      <td>0.052045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>135.772003</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>...</td>\n",
       "      <td>-389.764557</td>\n",
       "      <td>-330.183014</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>91312.648438</td>\n",
       "      <td>-71.102097</td>\n",
       "      <td>-869.824463</td>\n",
       "      <td>-752.000000</td>\n",
       "      <td>-2152.500000</td>\n",
       "      <td>4.269275e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cells_length  cells_max  cells_mean     cells_min  cells_var  elayer  \\\n",
       "0           4.0   0.341868    0.119788  2.044216e-02   0.017071     6.0   \n",
       "1           8.0   0.450497    0.071357  2.354972e-14   0.020859     2.0   \n",
       "2          11.0   0.680366    0.167157  1.636995e-02   0.035665     6.0   \n",
       "3           3.0   1.142238    0.461141  1.039750e-01   0.232131    12.0   \n",
       "4          10.0   0.791248    0.182531  3.750272e-02   0.052045     6.0   \n",
       "\n",
       "   emodule  evolume          ex          ey   ...          ymean     ymedian  \\\n",
       "0     29.0     14.0 -290.533997 -463.497009   ...    -256.535034 -286.803009   \n",
       "1    145.0     12.0 -231.250000  351.147003   ...     143.303238   92.862198   \n",
       "2     66.0     18.0  470.838989  768.648010   ...     378.134766  309.616516   \n",
       "3     26.0     18.0   94.234398 -865.289001   ...    -450.889038 -434.042480   \n",
       "4     27.0     16.0  135.772003 -984.747986   ...    -389.764557 -330.183014   \n",
       "\n",
       "         ymin           yvar         zmax        zmean      zmedian  \\\n",
       "0 -463.497009   26005.789062  1801.500000   951.502136  1031.050049   \n",
       "1   20.085199   10862.027344  -244.580994 -1334.507324  -958.000000   \n",
       "2   30.304199   72788.718750  1802.500000   782.384949   561.099976   \n",
       "3 -865.289001  130411.406250  2947.500000  1532.954224  1469.185059   \n",
       "4 -984.747986   91312.648438   -71.102097  -869.824463  -752.000000   \n",
       "\n",
       "          zmin          zvar event_id  \n",
       "0   107.703003  3.882222e+05        3  \n",
       "1 -2951.500000  7.111888e+05        3  \n",
       "2    64.440201  3.828446e+05        3  \n",
       "3   245.947006  1.497747e+06        3  \n",
       "4 -2152.500000  4.269275e+05        3  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for each track: ['cells_length' 'cells_max' 'cells_mean' 'cells_min' 'cells_var' 'elayer'\n",
      " 'emodule' 'evolume' 'ex' 'ey' 'ez' 'hitids' 'nclusters' 'nhits'\n",
      " 'nhitspercluster' 'nsensors' 'nvolumes' 'particle_id' 'sentence'\n",
      " 'sentence_count' 'sentence_value' 'slayer' 'smodule' 'svolume' 'sx' 'sy'\n",
      " 'sz' 'target' 'xmax' 'xmean' 'xmedian' 'xmin' 'xvar' 'ymax' 'ymean'\n",
      " 'ymedian' 'ymin' 'yvar' 'zmax' 'zmean' 'zmedian' 'zmin' 'zvar' 'event_id']\n"
     ]
    }
   ],
   "source": [
    "df_train=load_obj('files/df_train_v2-reduced.pkl')\n",
    "df_test=load_obj('files/df_test_v1.pkl')\n",
    "y_train=df_train.target.values\n",
    "y_test=df_test.target.values\n",
    "print(\"The dataframe with all features:\")\n",
    "display(df_train.head())\n",
    "print(\"Features for each track:\",df_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601fe0003c96bd293ef4dab881a52952bfc2e11c"
   },
   "source": [
    "In the competition, we used roughly 250 events for training, but the additional improvement to just using 13 events is not too big. We now create the LightGBM model, using the mentioned training data and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "d565ffb75b5d07695ee52de2830028a396133e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.954964\n",
      "[100]\tvalid_0's auc: 0.960705\n",
      "[150]\tvalid_0's auc: 0.962809\n",
      "[200]\tvalid_0's auc: 0.963605\n",
      "[250]\tvalid_0's auc: 0.964108\n",
      "[300]\tvalid_0's auc: 0.964196\n",
      "[350]\tvalid_0's auc: 0.964214\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's auc: 0.964268\n",
      "took 1.4954051971435547 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "s=time.time()\n",
    "# choose which features of the tracks we want to use:\n",
    "columns=['svolume','nclusters', 'nhitspercluster', 'xmax','ymax','zmax', 'xmin','ymin','zmin', 'zmean', 'xvar','yvar','zvar']\n",
    "rounds=1000\n",
    "round_early_stop=50\n",
    "parameters = { 'subsample_for_bin':800, 'max_bin': 512, 'num_threads':8, \n",
    "               'application': 'binary','objective': 'binary','metric': 'auc','boosting': 'gbdt',\n",
    "               'num_leaves': 128,'feature_fraction': 0.7,'learning_rate': 0.05,'verbose': 0}\n",
    "train_data = lightgbm.Dataset(df_train[columns].values, label=y_train)\n",
    "test_data = lightgbm.Dataset(df_test[columns].values, label=y_test)\n",
    "model = lightgbm.train(parameters,train_data,valid_sets=test_data,num_boost_round=rounds,early_stopping_rounds=round_early_stop,verbose_eval=50)\n",
    "print('took',time.time()-s,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9a7d872c9ae35b65913ec0251262570119ecd6d"
   },
   "source": [
    "### Judge machine learning model\n",
    "\n",
    "We doublecheck the model's performance, by calculating its precision, recall and accuracy on the validation set:\n",
    "\n",
    "[Note: For ML to be helpful in our situation, it needs to distinguish correct from wrong tracks with very high precision=true_positives/(false_positives+true_positives)). Also, it needs to do so for various sets of track candidates (especially such, which are generated if one tries to find tracks which originate far away from the origin; in those situations, often a lot of bad candidates are produced).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "e54c33c409e62e353b07f0ec981ad8cf8294a99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.1  --- Precision: 0.8458, Recall: 0.9955, Accuracy: 0.8861\n",
      "Threshold 0.5  --- Precision: 0.9060, Recall: 0.9608, Accuracy: 0.9150\n",
      "Threshold 0.9  --- Precision: 0.9621, Recall: 0.7437, Accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=model.predict(df_test[columns].values)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.1)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.5)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa41aa28075d5242c5aa1b6e054cfcdf5b83554"
   },
   "source": [
    "### Use machine learning model\n",
    "\n",
    "Calculate the probabilities for the tracks in those two submissions (small optimization: take also length of track into account, and after a couple of merged submissions, ask the probability of the track from the new submission to be at least C higher than the current probability; this latter option is not used in this kernel, but was used when merging >= 4 submissions). \n",
    "\n",
    "Then merge both submissions, based on the probabilities of its track candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "db72968ce0c1f3b4ed017ffed2d466196bfdf2f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfe897c53d642c3a01c995cd64902e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33927), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd1842d22ff45db960c777068539b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33756), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge submission 0 and 1 into sub01:\n",
      "Score: 0.5651194312399999\n"
     ]
    }
   ],
   "source": [
    "preds={}\n",
    "preds[1]=get_predictions(resa1,hits,model)\n",
    "preds[2]=get_predictions(resa2,hits,model)\n",
    "print('Merge submission 0 and 1 into sub01:')\n",
    "sub01=merge_with_probabilities(resa1,resa2,preds[1],preds[2],None,length_factor=0.5)\n",
    "score = score_event_fast(truth, sub01)\n",
    "print('Score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:07<00:00, 14.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f4a4cb207b4e8184be51386f93ad14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=6763.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6763/6763 [02:29<00:00, 45.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.6942199117400001\n"
     ]
    }
   ],
   "source": [
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "nresa=expand_tracks(sub01,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "nresa['event_id']=0\n",
    "score = score_event_fast(truth, nresa)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
