{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01baac3757b22b92747599ed87524c1ddc0ad1d"
   },
   "source": [
    "# Ultrafast sparse binning clustering enhanced by ML track scoring\n",
    "\n",
    "###  Yuval Reina, Tel-Aviv Israel, [Yuval.reina@gmail.com](Yuval.reina@gmail.com)\n",
    "###  Trian Xylouris, Frankfurt am Main Germany, [t.xylouris@gmail.com](t.xylouris@gmail.com)\n",
    "\n",
    "August 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competition Name: TrackML\n",
    "#### Team Name: Yuval & Trian\n",
    "#### Private Leaderboard Score: 0.80414\n",
    "#### Private Leaderboard Place: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We use sparse binning to perform ultrafast clustering. Tracks are first chosen according to their length, and later are scored and merged using a Machine Learning (=ML) algorithm. At the final stage the tracks are expanded by adding the closest hits to the track.\n",
    "\n",
    "**The biggest advantage of using clustering by sparse binning is speed**. This method can score 0.5 in just 40 sec using python on a single core, and it could be much faster if it was written in C++ and even faster by using paralleling on CPU or GPU. **The computational complexity of the clustering part in the algorithm is O(N)**. The feature calculation and binning can be done for every hit independently of the other hits and all hits are needed only for counting the number of hits in every bin. (The Python implementation uses np.unique which is actually O(NlogN)  ). We believe that with a careful implementation in C++  a score of 0.5 can be achieved in less than 10 seconds, allowing this algorithm to be an essential step in every fast algorithm (we will explain this statement later).\n",
    "\n",
    "The second stage in the algorithm is ML merging. The fastest way to merge 2 solutions is by assigning to each hit the track with the highest number of hits. This method is used in the clustering main loop. However, this method is limited because the number of hits is not always a good indication for a good track. The ML algorithm uses various features which describe the track and is able to distinguish between good and bad tracks. Unlike other ML solutions presented in Kaggle, our ML algorithm does not check the helix itself, as the clustering part already takes care of this.\n",
    "\n",
    "The last part in the algorithm is quite straightforward: track expanding is done by choosing the long tracks and adding to these tracks the closest loose hits (i.e. hits from short tracks). \n",
    "\n",
    "The full solution is then: \n",
    "•\tRun clustering a few times\n",
    "•\tUse ML to merge\n",
    "•\tExpand \n",
    "\n",
    "In the solution we submitted we used about 7 runs of clustering, using 100000 loop iterations each, merged and expanded twice (The 2nd time was just to add about 0.003 to the score by using a loophole in the definition of the competition metric [(1)](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using Sparse Binning\n",
    "\n",
    "The basic idea of clustering it to relate to every hit a set of numbers (features) and then to group hits with close feature together.\n",
    "The first stage in clustering is finding features which would describe a track and which we can calculate separately for every hit. \n",
    "We have published a post on criteria for good features [(2)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590).\n",
    "\n",
    "### Features \n",
    "Due to the magnetic field in the experiment, the particles form helixes, which travel along the z-axis. Each hit in the x-y-z space can be the member of any particular helix in a certain family of helixes. The idea of the algorithm is to go over each possible member of this family of helixes, and look at how many of all hits may belong to this particular member. The hits are then assigned to the helix (=track) with the most possible hits.\n",
    "\n",
    "We start by assuming the particle is formed in a small cylinder around the origin i.e. the approximate starting point of the helix is '(0,0,z0)'. (According to the introduction papers '|z0|<5.5mm').\n",
    "\n",
    "This kind of helix can be defined by 3 numbers, its Radius, its tangential angle at the origin in the xy plan (=the direction of the particle when it is created, in xy plan), the slop of the helix (how fast the particle moves in the Z direction compared to its velocity in xy).\n",
    "\n",
    "We will define:\n",
    "\n",
    "R – helix radius\n",
    "\n",
    "$\\theta$ – tangential angle in the xy plan\n",
    "\n",
    "$\\phi$ – Slope\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta = \\arctan\\frac{py}{px} \\label{x1} \\\\\n",
    "&\\phi = \\arctan\\frac{pz}{\\sqrt{px^2+py^2}} \\label{x2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where px, py, pz, are the particle’s initial momentum \n",
    "\n",
    "If we take a family of helixes with a radius R, we can calculate for every hit '(x,y,z)' the values for theta and phi.\n",
    "We define\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&kt = \\frac{1}{2R} \\label{x3} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We use kt>0 for particle rotating to the CW and kt<0 for CCW rotation\n",
    "\n",
    "Using sum trigonometry, we will get:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&\\theta\\_ = \\arctan\\frac{py}{px}\\label{x4} \\\\\n",
    "\\\\\n",
    "&rr=\\sqrt{x^2+y^2} \\label{x5} \\\\\n",
    "\\\\\n",
    "&\\Delta\\theta=\\arcsin{(kt\\cdot rr)} \\label{x6} \\\\\n",
    "\\\\\n",
    "&\\theta=\\theta\\_+\\Delta\\theta \\label{x7}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "(At this stage we ignore particles that rotate more then $\\pi$ radians)\n",
    "$$ \\begin{align*}\n",
    "&\\phi\\_ = \\arctan\\frac{(z-z0)\\cdot kt}{\\Delta\\theta} \\label{x8}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\theta$ and $\\phi\\_$ will be our features, but we need to tweak them a little to become useful.\n",
    "\n",
    "If $\\theta_1 = \\phi+\\epsilon$ and $\\theta_1 = \\phi-\\epsilon$,  $\\theta_1 – \\theta_2= 2\\epsilon$ on a circle, but if we calculate $\\theta_1 – \\theta_2$ we will get $2\\pi+2\\epsilon$. To solve this issue we will use 2 features instead:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "&sint = \\sin(\\theta) \\label{x9}\\\\\n",
    "&cost = \\cos(\\theta) \\label{x10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\phi\\_$ on the other hand is in the range $[-\\pi/2,\\pi/2]$, and does not have the above issue, but $\\phi\\_$ distribution is far from being uniform (for an extensive discussion about its distribution look here [(3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-371940)). \n",
    "\n",
    "The solution we found suitable is by defining:\n",
    "$$ \\begin{align*}\n",
    "&\\phi = \\arctan\\frac{(z-z0)\\cdot kt}{3.3\\cdot\\Delta\\theta}\\frac{2}{\\pi} \\label{x11}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(the last edition is for scaling to a [-1,1] range)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Binning Clustering\n",
    "Binning is the easiest way to cluster, in this method we divide the values of each feature to constant areas and a cluster is formed by all the hits whos features are all in the same area. \n",
    "\n",
    "As an example, if we have one feature in the range `[-1,1]` we can decide to divide it to 20 equal bins of size `0.1: [-1,-0.9), [-0.9,0.8), …,[0.9,1]`. We will assign every bin a number 0 , …, 19 and define the hit’s feature with this value. If we have 3 features for a hit we will define that hit with the 3 values of bins. As an example:\n",
    "\n",
    "`\n",
    "Hit #1 – (12,15,0)\n",
    "Hit #2 – (11,5,2)`\n",
    "\n",
    "Two hits which have the exact 3 values, are clustered together to form track candidate.\n",
    "\n",
    "In general, the bins don’t have to spread uniformly, and every feature can have a different number of bins.\n",
    "\n",
    "In Sparse Binning we use a very simple binning algorithm. We start by using uniform spread bins which can be easily calculated:\n",
    "\n",
    "For feature `F1` in the range `[-1,1]` we define `B1 = int(F1*k1)` where the number of bins is `2*k1`\n",
    "\n",
    "i.e. we multiply the feature by half the number of bins and take the integer part.\n",
    "\n",
    "To combine few features together and get the cluster id (track_id) we just offset each feature by multiplying it by a number which is big enough. \n",
    "\n",
    "If we have 2 features F1, F2 and we want to have 2*k1, 2*k2 bins for each feature, we can define:\n",
    "\n",
    "` (a) Track_id = int(F1*k1) + (2*k1+1)*int(F2*k2)`\n",
    "\n",
    "And that’s it clustering is finished!!\n",
    "\n",
    "This calculation takes about 4mSec for 120,000 hits on Kaggle kernel platform compared to about 950mSec for dbscan which is the most popular clustering algorithm used by the competitors in the TrackML Challenge. \n",
    "\n",
    "This type of clustering moves the bottleneck of the full solution, form clustering to merging (will be discussed in the next section).\n",
    "\n",
    "One weakness of the binning algorithm is its sensitivity caused, among other things, by the hard boarders between bins. As an example, let’s take two hits:\n",
    "\n",
    "`\n",
    "Hit1 – with features (-1e-7,0.1,0.3)\n",
    "Hit2 – with features (1e-7,0.1,0.3)`\n",
    "\n",
    "If we use the calculation above (1), Hit1, Hit2 will never cluster together no matter how many bins we’ll have.\n",
    "\n",
    "This issue is solved by adding a random number in the range `[0,1)` to every feature in the binning calculation, making (a):\n",
    "\n",
    "`(b) Track_id = int(F1*k1+r1) + (2*k1+1)*int(F2*k2+r2)`\n",
    "\n",
    "Where r1, r2 are random numbers with uniform distribution in the range `[0,1)`. r1, r2 are changed every time we cluster.\n",
    "\n",
    "When compared to dbscan, sparse clustering has one big disadvantage, it’s sensitivity, it can miss some of the hits because they fall in another bin. This sensitivity is also its strength, as it doesn’t cluster many wrong hits (outliners) which degrade the maximum achievable score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Merging\n",
    "Every time we cluster with a set of kt, z0, every hit gets a new track_id, to have a full solution we want to merge two lists of track id’s together. The most efficient way we found was to measure the length of each track in the two lists, and let every hit choose the longest track it can.\n",
    "\n",
    "If Hit #1 has track_id=1 in list #100 and track_id=10000002 in list #101 (the track_id’s in both lists should be completely different) and in list #100 there are 10 hits with track_id=1 while in list #101 there are 7 hits with track_id=10000002, hit#1 will choose track_id=1.\n",
    "\n",
    "To do this, we need to measure the track’s length. The fastest way we know to do this in Python is by using numpy.unique. This operation takes 11mSec for 120000 hits on Kaggle kernel platform. The numpy.unique computational complexity is O(NlogN) because it sorts the values. While if we have enough memory, this task can easily be done in O(N). (We can also reduce the amount of memory needed by hashing). \n",
    "\n",
    "In our main loop we measure the length of a track twice, the first time we do as we described above to create a track candidate and then we measure again to make sure the track is still long enough as hits may decide to choose another track in the first round. \n",
    "\n",
    "The double track length measurement makes this the bottleneck of the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering – the full algorithm\n",
    "The algorithm iterate over random selected pairs – (z0,kt). For every pair the hits’ features are calculated, and clustering is performed by the sparse binning algorithm. The new clustering result is then merged with previous ones with the merging algorithm described above. \n",
    "\n",
    "Every 500 loops an extra step is taken. Tracks are examined for hits coming from the same detector (equal Volume_id, Layer_id, and Module_id) or from the same layer. There can’t be two hits from the same detector or 3 from the same layer. The extra hits, are discovered by measuring the distance between the hit and the center of gravity of the track. These hits are removed from the track. \n",
    "\n",
    "In this step the algorithm also permanently set tracks which are long enough (hits won’t we taken away or added to this track any more).\n",
    "\n",
    "While running the algorithm can change the number of bins per feature and the length of the minimal track to set, these settings are user defined parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering results:\n",
    "The score common score after the clustering algorithm with as a function of  the number of (z0,kt) pairs used is presented in the following table (measured on event 000001000 from the training set – usually 0.015 below final LB score):\n",
    "\n",
    "|   Pairs   |  Score |\n",
    "|-----------|--------|\n",
    "|    1000\t|  0.51  |\n",
    "|    1600\t|  0.56  |\n",
    "|    5500\t|  0.636 |\n",
    "|  100000\t|  0.73  |\n",
    "\n",
    "The score plateau after about 90000 pairs and wouldn’t get higher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning\n",
    "Running the clustering algorithm a few times will produce similar, but slightly different solutions. Every one of these solutions has some good tracks that the other solution missed. Trying to marge these solutions in a naïve way, by selecting the longest track for each hit, does not improve the score when the initial scores are good enough.\n",
    "\n",
    "In our solution we built an algorithm to merge few solutions by using a Machine Learning (=ML) algorithm to evaluate the quality of the tracks. Although there were some discussions on taking a similar approach [(4)](https://www.kaggle.com/c/trackml-particle-identification/discussion/58323), we don’t know of anyone who implemented it. As far as we know, most ML implementations where in an effort to construct or expand the tracks. \n",
    "\n",
    "The nice thing about this kind of implementation is the fact that it is quite agnostic to the track construction algorithm, hence it can be used to merge tracks from completely different algorithms. \n",
    "\n",
    "### General strategy\n",
    "The general strategy for this solution is as follows: \n",
    "- Produce different submission candidates sub_1, sub_2, ..., sub_N\n",
    "- Create a machine learning model, which gives probabilities between 0 and 1 for each track candidate\n",
    "- Merge two submission candidates by assigning to each hit the track, which has higher probability\n",
    "- Merge all submission candidates to get the final submission. The merging can be done sequentially, as we did in our final submission \n",
    "\n",
    "### Creating the machine learning model\n",
    "The machine learning model we use is LightGBM. We chose 13 features per track:\n",
    "- variance of x,y,z (these are the most important)\n",
    "- minimum of x,y,z\n",
    "- maximum of x,y,z\n",
    "- mean of z\n",
    "- volume_id of first hit\n",
    "- number of clusters per track (i.e. are there many hits, which are close together?)\n",
    "- number of hits divided by number of clusters\n",
    "We tried many more features (e.g. number of different volumes crossed, means of x,y etc.), but we saw only negligible gains, probably because these features are closely related to the features we already use. \n",
    "\n",
    "### Training and validation\n",
    "The model was trained using 250 events from the training set. The true tracks (target=1) were selected using the truth file. The wrong tracks (target=0) were generated by first running the clustering algorithm on an event, and then picking all generated tracks, whose hits belonged to at least two different particles (particle_id). \n",
    "\n",
    "Validation was done using the 3 training events with the ids 0, 1 and 2.\n",
    "\n",
    "### Results\n",
    "In our final solution we used the algorithm to sequentially merge 7 clustering solutions (mostly from previously generated submissions). We were able to increase out score by about 0.01.\n",
    "After the completion of the competition, we did another test where we merge 64 fast clustering solutions, using 1000 (z0,kt) pairs each (i.e. a total of 64000 pairs). The score we got after expanding, for train event 1000, was 0.782. We get a similar score, but with longer runtime, by our usual algorithm of clustering 100000 pairs expansion, which is slower. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Track Extension\n",
    "The major drawback of the binning clustering algorithm is its sensitivity. The bins sharp boarders can leave a hit out of the track although it is very close to the other hits. To overcome this issue, a track expanding algorithm is the final step in the full solution.\n",
    "\n",
    "The final clustering and merging solution it taken as a basis for this step.\n",
    "\n",
    "First, the algorithm tries to improve the (z0,kt) pair for each track. It does this by searching for a pair which will minimize the standard deviation of the track hits.\n",
    "\n",
    " Using these refined (kt,z0) values. The hits which are closest to track are added to it. The distance between a hit and a track is measured by the difference between the its calculated features and the mean of the tracks features.\n",
    " \n",
    "We also tested a variant of this algorithm which measure the minimal distance to one of the track’s hit (as in nearest neighbor algorithm), this variant performed slightly better but was much slower.  \n",
    "\n",
    "The improvement gained by expanding depends on the score after the previous stages.\n",
    "\n",
    "0.63 score can jump to 0.73,\n",
    "\n",
    "0.73 would go to 0.79\n",
    "\n",
    "and in our final run 0.78 was improved to 0.804\n",
    "\n",
    "This input to this stage must come from good clustering algorithm and cascaded extending will usually degrade the score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "1.\tAs a full solution our approach can be easily optimized for speed:\n",
    "    -\tThe clustering algorithm can be improved as described above. Farther improvement for real world application can be achieved by massively paralleling the feature calculation using a GPU (as we explained above, every hit is treated separately, which makes the algorithm very suitable for parallel processing).\n",
    "    -\tBoth the ML and the expanding algorithm where not optimized at all as they weren’t the bottleneck of the solution. As an example, the expanding algorithm recalculates the features for all the loose hits every time. Calculating the features only for hits with z>0 while expanding tracks with z>0 will immediately cut the running time of the algorithm by half. \n",
    "2.\tElements of our approach may be beneficial for any final solution to this problem.\n",
    "   - In particular, our binning algorithm creates a lot of good candidate tracks in a short period of time. It may be helpful to employ this at the beginning of a solution, in order to very quickly (<1 minute) detect 50% of all tracks (use small bins). One can then continue with running a different algorithm on the remaining hits.\n",
    "   - Similarly, the machine learning algorithm, as well as the employed parameters and track extension algorithm may improve any final result, while just adding minutes (or less, after optimization) to the total runtime.\n",
    "3.\tIn our algorithm we didn’t do any adjustments for uneven magnetic field. If we incorporate @CPMP’s findings [here (3)](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564). The score of the algorithm improves immediately by 0.02 to about 0.82. \n",
    "4.\tAn obvious short-coming of   main approach is, that it is not suited to find tracks, which originate far from the origin. We did try to adjust our algorithm to work also in that situation, but, had minimal success so far.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "(1)  @Grzegorz Sionkowski comment in *\"how to score a track is good or not\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053](https://www.kaggle.com/c/trackml-particle-identification/discussion/60638#354053)\n",
    "\n",
    "(2) *\"Criteria for good features\"* [(https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)](https://www.kaggle.com/c/trackml-particle-identification/discussion/61590)\n",
    "\n",
    "(3) @CPMP *\"Solution #9\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564](https://www.kaggle.com/c/trackml-particle-identification/discussion/63250#latest-372564)\n",
    "\n",
    "(4) @Heng CherKeng *\"ensemble clustering?\"* [https://www.kaggle.com/c/trackml-particle-identification/discussion/58323](https://www.kaggle.com/c/trackml-particle-identification/discussion/58323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Running full pipeline for train event 1000\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We will demonstrate how to get a score of TBD for train event 1000, in TBD minutes, by using our aforementioned method. The steps are: \n",
    "- Create 2 initial solutions: 2x use 5.500 pairs of (kt,z0) for binning (in our original solution, we use 100.000 pairs)\n",
    "- Merge the 2 solutions using a machine learning algorithm (in our original solution, we use 8)\n",
    "- Extend the tracks (in our original solution, we run this twice, while at the first time, we extend hits according to whether their values for (1) and (2) are close enough to at least 1 hit, instead of the mean of the track)\n",
    "\n",
    "Import necessary packages and load train event 1000, which will be the event we will be working on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'other/')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from ipywidgets import FloatProgress,FloatText\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import gc\n",
    "import cProfile\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)})\n",
    "plt.figure(figsize=(12,5))\n",
    "path='files/'\n",
    "\n",
    "from functions.other import calc_features, get_event, score_event_fast, load_obj\n",
    "from functions.expand import *\n",
    "from functions.cluster import *\n",
    "from functions.ml_model import merge_with_probabilities,precision_and_recall,get_features,get_predictions\n",
    "\n",
    "# the following two lines are for changing imported functions, and not needing to restart kernel to use their updated version\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "event_num=0\n",
    "event_prefix = 'event00000100{}'.format(event_num)\n",
    "hits, cells, particles, truth = get_event(path,event_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Define parameters and run clustering, twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "00cff1de64d7a7f24df2e9430344f5ae90dc953c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d13ab9dbba412fbcc2a0ffe29b9ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5ecfc6aaf94da891f6257b26bae7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5450580b6f466e954db5d6b438db14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e1a030a79c4aaf817aab1d4e8977aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0332c8c13d2d4dcb9448f60f3604868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313c1e05c99144a584769256a243d742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd75bce81f7249f49f6b24e379ee3bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:30<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 270.45298 sec\n",
      "Your score:  0.63886539606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e006e038e74b1ebc1f397271b4b4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e656d456055401aa992f93e3fad728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5d03564b1047fbbc243b26248d65da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc877b8a740a47528732a95074ff5d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f298b61ce641412480bbe5f1d4d406ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04d16ae11e541078b8870fbfdcb6f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a34199b6fe4428687f038ba227eb0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:25<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 265.83738 sec\n",
      "Your score:  0.63807359373\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "weights={'pi':1,'theta':0.15}\n",
    "stds={'z0':7.5, 'kt':7.5e-4}\n",
    "d =    {'sint':[225,110,110,110,110,110],\n",
    "        'cost':[225,110,110,110,110,110],\n",
    "          'phi':[550,260,260,260,260,260],\n",
    "        'min_group':[11,11,10,9,8,7],\n",
    "        'npoints':[500,2000,1000,1000,500,500]}\n",
    "filters=pd.DataFrame(d)\n",
    "nu=500\n",
    "\n",
    "resa1=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa1[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa1.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)\n",
    "\n",
    "resa2=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "resa2[\"event_id\"]=event_num\n",
    "score = score_event_fast(truth, resa2.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317879ec4824598fad62b7faba8f981eed065681"
   },
   "source": [
    "## Employ Machine Learning\n",
    "\n",
    "[Note: For our final solution, the methods described in this chapter gave around +0.01 to the LB score. Certain benefits from these methods have already been captured by the function, which expands the tracks.]\n",
    "\n",
    "We have prepared the training and test data and load it directly from a pkl-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5800822ec8d5798c6bc9ecba2df5f20964fe13cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with all features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cells_length</th>\n",
       "      <th>cells_max</th>\n",
       "      <th>cells_mean</th>\n",
       "      <th>cells_min</th>\n",
       "      <th>cells_var</th>\n",
       "      <th>elayer</th>\n",
       "      <th>emodule</th>\n",
       "      <th>evolume</th>\n",
       "      <th>ex</th>\n",
       "      <th>ey</th>\n",
       "      <th>...</th>\n",
       "      <th>ymean</th>\n",
       "      <th>ymedian</th>\n",
       "      <th>ymin</th>\n",
       "      <th>yvar</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zmedian</th>\n",
       "      <th>zmin</th>\n",
       "      <th>zvar</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.341868</td>\n",
       "      <td>0.119788</td>\n",
       "      <td>2.044216e-02</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-290.533997</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>...</td>\n",
       "      <td>-256.535034</td>\n",
       "      <td>-286.803009</td>\n",
       "      <td>-463.497009</td>\n",
       "      <td>26005.789062</td>\n",
       "      <td>1801.500000</td>\n",
       "      <td>951.502136</td>\n",
       "      <td>1031.050049</td>\n",
       "      <td>107.703003</td>\n",
       "      <td>3.882222e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.450497</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>2.354972e-14</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-231.250000</td>\n",
       "      <td>351.147003</td>\n",
       "      <td>...</td>\n",
       "      <td>143.303238</td>\n",
       "      <td>92.862198</td>\n",
       "      <td>20.085199</td>\n",
       "      <td>10862.027344</td>\n",
       "      <td>-244.580994</td>\n",
       "      <td>-1334.507324</td>\n",
       "      <td>-958.000000</td>\n",
       "      <td>-2951.500000</td>\n",
       "      <td>7.111888e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.680366</td>\n",
       "      <td>0.167157</td>\n",
       "      <td>1.636995e-02</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>470.838989</td>\n",
       "      <td>768.648010</td>\n",
       "      <td>...</td>\n",
       "      <td>378.134766</td>\n",
       "      <td>309.616516</td>\n",
       "      <td>30.304199</td>\n",
       "      <td>72788.718750</td>\n",
       "      <td>1802.500000</td>\n",
       "      <td>782.384949</td>\n",
       "      <td>561.099976</td>\n",
       "      <td>64.440201</td>\n",
       "      <td>3.828446e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.142238</td>\n",
       "      <td>0.461141</td>\n",
       "      <td>1.039750e-01</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.234398</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>...</td>\n",
       "      <td>-450.889038</td>\n",
       "      <td>-434.042480</td>\n",
       "      <td>-865.289001</td>\n",
       "      <td>130411.406250</td>\n",
       "      <td>2947.500000</td>\n",
       "      <td>1532.954224</td>\n",
       "      <td>1469.185059</td>\n",
       "      <td>245.947006</td>\n",
       "      <td>1.497747e+06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.791248</td>\n",
       "      <td>0.182531</td>\n",
       "      <td>3.750272e-02</td>\n",
       "      <td>0.052045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>135.772003</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>...</td>\n",
       "      <td>-389.764557</td>\n",
       "      <td>-330.183014</td>\n",
       "      <td>-984.747986</td>\n",
       "      <td>91312.648438</td>\n",
       "      <td>-71.102097</td>\n",
       "      <td>-869.824463</td>\n",
       "      <td>-752.000000</td>\n",
       "      <td>-2152.500000</td>\n",
       "      <td>4.269275e+05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cells_length  cells_max  cells_mean     cells_min  cells_var  elayer  \\\n",
       "0           4.0   0.341868    0.119788  2.044216e-02   0.017071     6.0   \n",
       "1           8.0   0.450497    0.071357  2.354972e-14   0.020859     2.0   \n",
       "2          11.0   0.680366    0.167157  1.636995e-02   0.035665     6.0   \n",
       "3           3.0   1.142238    0.461141  1.039750e-01   0.232131    12.0   \n",
       "4          10.0   0.791248    0.182531  3.750272e-02   0.052045     6.0   \n",
       "\n",
       "   emodule  evolume          ex          ey   ...          ymean     ymedian  \\\n",
       "0     29.0     14.0 -290.533997 -463.497009   ...    -256.535034 -286.803009   \n",
       "1    145.0     12.0 -231.250000  351.147003   ...     143.303238   92.862198   \n",
       "2     66.0     18.0  470.838989  768.648010   ...     378.134766  309.616516   \n",
       "3     26.0     18.0   94.234398 -865.289001   ...    -450.889038 -434.042480   \n",
       "4     27.0     16.0  135.772003 -984.747986   ...    -389.764557 -330.183014   \n",
       "\n",
       "         ymin           yvar         zmax        zmean      zmedian  \\\n",
       "0 -463.497009   26005.789062  1801.500000   951.502136  1031.050049   \n",
       "1   20.085199   10862.027344  -244.580994 -1334.507324  -958.000000   \n",
       "2   30.304199   72788.718750  1802.500000   782.384949   561.099976   \n",
       "3 -865.289001  130411.406250  2947.500000  1532.954224  1469.185059   \n",
       "4 -984.747986   91312.648438   -71.102097  -869.824463  -752.000000   \n",
       "\n",
       "          zmin          zvar event_id  \n",
       "0   107.703003  3.882222e+05        3  \n",
       "1 -2951.500000  7.111888e+05        3  \n",
       "2    64.440201  3.828446e+05        3  \n",
       "3   245.947006  1.497747e+06        3  \n",
       "4 -2152.500000  4.269275e+05        3  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for each track: ['cells_length' 'cells_max' 'cells_mean' 'cells_min' 'cells_var' 'elayer'\n",
      " 'emodule' 'evolume' 'ex' 'ey' 'ez' 'hitids' 'nclusters' 'nhits'\n",
      " 'nhitspercluster' 'nsensors' 'nvolumes' 'particle_id' 'sentence'\n",
      " 'sentence_count' 'sentence_value' 'slayer' 'smodule' 'svolume' 'sx' 'sy'\n",
      " 'sz' 'target' 'xmax' 'xmean' 'xmedian' 'xmin' 'xvar' 'ymax' 'ymean'\n",
      " 'ymedian' 'ymin' 'yvar' 'zmax' 'zmean' 'zmedian' 'zmin' 'zvar' 'event_id']\n"
     ]
    }
   ],
   "source": [
    "df_train=load_obj('files/df_train_v2-reduced.pkl')\n",
    "df_test=load_obj('files/df_test_v1.pkl')\n",
    "y_train=df_train.target.values\n",
    "y_test=df_test.target.values\n",
    "print(\"The dataframe with all features:\")\n",
    "display(df_train.head())\n",
    "print(\"Features for each track:\",df_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601fe0003c96bd293ef4dab881a52952bfc2e11c"
   },
   "source": [
    "In the competition, we used roughly 250 events for training, but the additional improvement to just using 13 events is not too big. We now create the LightGBM model, using the mentioned training data and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d565ffb75b5d07695ee52de2830028a396133e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.954964\n",
      "[100]\tvalid_0's auc: 0.960705\n",
      "[150]\tvalid_0's auc: 0.962809\n",
      "[200]\tvalid_0's auc: 0.963605\n",
      "[250]\tvalid_0's auc: 0.964108\n",
      "[300]\tvalid_0's auc: 0.964196\n",
      "[350]\tvalid_0's auc: 0.964214\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's auc: 0.964268\n",
      "took 3.1330173015594482 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "s=time.time()\n",
    "# choose which features of the tracks we want to use:\n",
    "columns=['svolume','nclusters', 'nhitspercluster', 'xmax','ymax','zmax', 'xmin','ymin','zmin', 'zmean', 'xvar','yvar','zvar']\n",
    "rounds=1000\n",
    "round_early_stop=50\n",
    "parameters = { 'subsample_for_bin':800, 'max_bin': 512, 'num_threads':8, \n",
    "               'application': 'binary','objective': 'binary','metric': 'auc','boosting': 'gbdt',\n",
    "               'num_leaves': 128,'feature_fraction': 0.7,'learning_rate': 0.05,'verbose': 0}\n",
    "train_data = lightgbm.Dataset(df_train[columns].values, label=y_train)\n",
    "test_data = lightgbm.Dataset(df_test[columns].values, label=y_test)\n",
    "model = lightgbm.train(parameters,train_data,valid_sets=test_data,num_boost_round=rounds,early_stopping_rounds=round_early_stop,verbose_eval=50)\n",
    "print('took',time.time()-s,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9a7d872c9ae35b65913ec0251262570119ecd6d"
   },
   "source": [
    "### Judge machine learning model\n",
    "\n",
    "We doublecheck the model's performance, by calculating its precision, recall and accuracy on the validation set:\n",
    "\n",
    "[Note: For ML to be helpful in our situation, it needs to distinguish correct from wrong tracks with very high precision=true_positives/(false_positives+true_positives)). Also, it needs to do so for various sets of track candidates (especially such, which are generated if one tries to find tracks which originate far away from the origin; in those situations, often a lot of bad candidates are produced).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e54c33c409e62e353b07f0ec981ad8cf8294a99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.1  --- Precision: 0.8458, Recall: 0.9955, Accuracy: 0.8861\n",
      "Threshold 0.5  --- Precision: 0.9060, Recall: 0.9608, Accuracy: 0.9150\n",
      "Threshold 0.9  --- Precision: 0.9621, Recall: 0.7437, Accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=model.predict(df_test[columns].values)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.1)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.5)\n",
    "precision, recall, accuracy=precision_and_recall(y_test, y_test_pred,threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa41aa28075d5242c5aa1b6e054cfcdf5b83554"
   },
   "source": [
    "### Use machine learning model\n",
    "\n",
    "Calculate the probabilities for the tracks in those two submissions (small optimization: take also length of track into account, and after a couple of merged submissions, ask the probability of the track from the new submission to be at least C higher than the current probability; this latter option is not used in this kernel, but was used when merging >= 4 submissions). \n",
    "\n",
    "Then merge both submissions, based on the probabilities of its track candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "db72968ce0c1f3b4ed017ffed2d466196bfdf2f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2857a63537624f6cb5ba080bd3d02b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29497), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d386856973834f50a2464f2abaa825ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge submission 0 and 1 into sub01:\n",
      "Score: 0.6795995880399999\n"
     ]
    }
   ],
   "source": [
    "preds={}\n",
    "preds[1]=get_predictions(resa1,hits,model)\n",
    "preds[2]=get_predictions(resa2,hits,model)\n",
    "print('Merge submission 0 and 1 into sub01:')\n",
    "sub01=merge_with_probabilities(resa1,resa2,preds[1],preds[2],None,length_factor=0.5)\n",
    "score = score_event_fast(truth, sub01)\n",
    "print('Score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.16it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d121857b5e74e98b6a71ddab9bb4bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=7296.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7296/7296 [03:48<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.7542627652699999\n"
     ]
    }
   ],
   "source": [
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "nresa=expand_tracks(sub01,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "nresa['event_id']=0\n",
    "score = score_event_fast(truth, nresa)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary merge 4*5500pairs clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d925919744452690c25558540d8c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bf1189cda34e328930dc62f705ff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9aa574794447fa8452fe9ff669dc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798e4ee5beef4619ad3e5e82eeb0bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d50ff865f3048ccb8103ff359b3b437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f2a658cc1440bc8ffb745f346d365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ece5460bbdc45b892f31f8cf654aacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:26<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 266.46599 sec\n",
      "Your score:  0.63753683302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e5ca81279a440494cba191a7459436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8020bfaea69b41de87a1c546e7992510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faffde2d9fc24924bb5003cd4340dc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8a15ece9c840179eab56c081ffc98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5be6e2ea9a1483fa1d2a7ed1fecf5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb54d3a81654ad6ab8245aa295a95bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298b2f7459934279bbc56cc575c42f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:25<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 265.51817 sec\n",
      "Your score:  0.6370084089700001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b7c491eca6419ca75d8f8a0d910896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d2487b7a3f4589816e68e71a185b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29498), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge submission stage: 1\n",
      "Score: 0.6778462493099999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46b2ce840c0465885080719246b7b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640c14b37b94483b8758953a6f0f26a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18562f893f6545a2955fccc350340f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c70f12cb398431cab983f5132905a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06185b072ae5487995da6ac88271b81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f3d1db409d40058854045685425890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516eb61b30ba4a3ab6b22add1578d949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:27<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 268.20304 sec\n",
      "Your score:  0.63557090396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e747d9a05cdd4205a91586e23494fbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='full score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e291dd9cb4e4a6e8adb4d79938c1ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e2f2c00d8e4b12ab6cdcabe81043b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='s rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce9847746a04cf0a14354a5dcea67a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='add score:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527110c4ec5745f5a5b8f31a078b7a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Rest size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbe68f952764349aa205753421e1745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=120939.0, description='Group size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7435eaf97e22479a930caadbc6841f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='filter:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [04:25<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 265.36214 sec\n",
      "Your score:  0.64030075506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fec4bdfb6343798b5eebc07ac2e36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffdaf64167243dc817a548f8db3d6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29415), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge submission stage: 1\n",
      "Score: 0.67806603201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b15fe97ab4446cbed00984af885d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31768), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec9c5983a9d4d32ba0c29c1729022be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31649), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge submission stage: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.71003237628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f6a38012524a86b597bd5d1d6d859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='calculating:', max=7370.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7370/7370 [03:42<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.7689132139700001\n"
     ]
    }
   ],
   "source": [
    "def binary_cluster_merege(stage):\n",
    "    if stage==0:\n",
    "        weights={'pi':1,'theta':0.15}\n",
    "        stds={'z0':7.5, 'kt':7.5e-4}\n",
    "        d =    {'sint':[225,110,110,110,110,110],\n",
    "                'cost':[225,110,110,110,110,110],\n",
    "                  'phi':[550,260,260,260,260,260],\n",
    "                'min_group':[11,11,10,9,8,7],\n",
    "                'npoints':[500,2000,1000,1000,500,500]}\n",
    "        filters=pd.DataFrame(d)\n",
    "        nu=500\n",
    "        res=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\n",
    "        res[\"event_id\"]=event_num\n",
    "        score = score_event_fast(truth, res.rename(index=str, columns={\"label\": \"track_id\"}))\n",
    "        print(\"Your score: \", score)\n",
    "    else:\n",
    "        res1=binary_cluster_merege(stage-1)\n",
    "        res2=binary_cluster_merege(stage-1)\n",
    "        preds1=get_predictions(res1,hits,model)\n",
    "        preds2=get_predictions(res2,hits,model)\n",
    "        print('Merge submission stage:',stage)\n",
    "        res=merge_with_probabilities(res1,res2,preds1,preds2,None,length_factor=0.5)\n",
    "        score = score_event_fast(truth, res)\n",
    "        print('Score:',score)\n",
    "    return(res)\n",
    "\n",
    "res = binary_cluster_merege(2)\n",
    "mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\n",
    "mstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "weights={'theta':0.1, 'phi':1}\n",
    "res=expand_tracks(res,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=100)\n",
    "res['event_id']=0\n",
    "score = score_event_fast(truth, res)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
